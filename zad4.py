#!/usr/bin/env python3
"""
S01E05 - Cenzura danych agent√≥w przez LLM
Cenzuruje imiƒô i nazwisko, wiek, miasto oraz ulicƒô+numer,
zastƒôpujƒÖc je s≈Çowem "CENZURA" wy≈ÇƒÖcznie przez LLM.
Obs≈Çuga: openai, lmstudio, anything, gemini, claude.
DODANO: Obs≈Çugƒô Claude + liczenie token√≥w i koszt√≥w dla wszystkich silnik√≥w (bezpo≈õrednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""

import argparse
import os
import sys

import requests
from dotenv import load_dotenv

load_dotenv(override=True)

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(description="Cenzura danych (multi-engine + Claude)")
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

# POPRAWKA: Lepsze wykrywanie silnika (jak w poprawionych zad1.py i zad2.py)
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # Pr√≥buj wykryƒá silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # Sprawd≈∫ kt√≥re API keys sƒÖ dostƒôpne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domy≈õlnie

print(f"üîÑ ENGINE wykryty: {ENGINE}")

# Sprawdzenie czy silnik jest obs≈Çugiwany
if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"‚ùå Nieobs≈Çugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"‚úÖ Engine: {ENGINE}")

CENTRALA_API_KEY = os.getenv("CENTRALA_API_KEY")
REPORT_URL = os.getenv("REPORT_URL")
CENZURA_URL = os.getenv("CENZURA_URL")

if not CENTRALA_API_KEY or not REPORT_URL or not CENZURA_URL:
    print("‚ùå Brak ustawienia CENTRALA_API_KEY, REPORT_URL lub CENZURA_URL w .env")
    sys.exit(1)


def download_text(url: str) -> str:
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.text.strip()
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas pobierania danych: {e}")
        sys.exit(1)


# --- ULTRA-TWARDY PROMPT ---
PROMPT_SYSTEM = (
    "Jeste≈õ automatem do cenzury danych osobowych w jƒôzyku polskim. "
    "NIE WOLNO Ci zmieniaƒá ≈ºadnych innych s≈Ç√≥w, znak√≥w interpunkcyjnych, uk≈Çadu tekstu ani zamieniaƒá kolejno≈õci zda≈Ñ. "
    "Zamie≈Ñ TYLKO i WY≈ÅƒÑCZNIE:\n"
    "- ka≈ºde imiƒô i nazwisko na 'CENZURA',\n"
    "- ka≈ºdƒÖ nazwƒô miasta na 'CENZURA',\n"
    "- ka≈ºdƒÖ nazwƒô ulicy wraz z numerem domu/mieszkania na 'CENZURA',\n"
    "- ka≈ºdƒÖ informacjƒô o wieku (np. '45 lat', 'wiek: 32', 'lat 27', 'ma 29 lat') na 'CENZURA'.\n"
    "Nie wolno parafrazowaƒá, nie wolno podsumowywaƒá, nie wolno streszczaƒá ani zamieniaƒá kolejno≈õci czegokolwiek. "
    "Wynikowy tekst musi mieƒá identyczny uk≈Çad, interpunkcjƒô i liczbƒô linii jak orygina≈Ç. "
    "Ka≈ºda inna zmiana ni≈º cenzura wy≈ºej powoduje b≈ÇƒÖd i NIEZALICZENIE zadania. "
    "Nie pisz ≈ºadnych komentarzy, nie wyja≈õniaj odpowiedzi. "
    "ODPOWIEDZ WY≈ÅƒÑCZNIE TEKSTEM Z OCENZURƒÑ. "
    "PRZYK≈ÅAD:\n"
    "Orygina≈Ç:\n"
    "Dane podejrzanego: Jan Kowalski, lat 45, mieszka w Krakowie, ul. Polna 8.\n"
    "Wyj≈õcie:\n"
    "Dane podejrzanego: CENZURA, lat CENZURA, mieszka w CENZURA, ul. CENZURA."
)


def censor_llm(text: str) -> str:
    prompt_user = (
        "Tekst do cenzury (nie zmieniaj nic poza danymi osobowymi, przyk≈Çad wy≈ºej!):\n"
        + text
    )

    # --- OpenAI ---
    if ENGINE == "openai":
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_OPENAI", "gpt-4o-mini"
        )

        if not OPENAI_API_KEY:
            print("‚ùå Brak OPENAI_API_KEY", file=sys.stderr)
            sys.exit(1)

        try:
            from openai import OpenAI
        except ImportError:
            print("‚ùå Musisz zainstalowaƒá openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie token√≥w
        tokens = resp.usage
        cost = (
            tokens.prompt_tokens / 1_000_000 * 0.60
            + tokens.completion_tokens / 1_000_000 * 2.40
        )
        print(
            f"[üìä Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[üí∞ Koszt OpenAI: {cost:.6f} USD]")
        return resp.choices[0].message.content.strip()

    # --- Claude ---
    elif ENGINE == "claude":
        CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514"
        )

        if not CLAUDE_API_KEY:
            print(
                "‚ùå Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr
            )
            sys.exit(1)

        try:
            from anthropic import Anthropic
        except ImportError:
            print(
                "‚ùå Musisz zainstalowaƒá anthropic: pip install anthropic",
                file=sys.stderr,
            )
            sys.exit(1)

        claude_client = Anthropic(api_key=CLAUDE_API_KEY)
        resp = claude_client.messages.create(
            model=MODEL_NAME,
            messages=[
                {"role": "user", "content": PROMPT_SYSTEM + "\n\n" + prompt_user}
            ],
            temperature=0,
            max_tokens=4000,
        )

        # Liczenie token√≥w Claude
        usage = resp.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(
            f"[üìä Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]"
        )
        print(f"[üí∞ Koszt Claude: {cost:.6f} USD]")

        return resp.content[0].text.strip()

    # --- Gemini (Google) ---
    elif ENGINE == "gemini":
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_GEMINI", "gemini-2.5-pro-latest"
        )

        if not GEMINI_API_KEY:
            print("‚ùå Brak GEMINI_API_KEY w .env", file=sys.stderr)
            sys.exit(1)

        try:
            import google.generativeai as genai
        except ImportError:
            print(
                "‚ùå Musisz zainstalowaƒá google-generativeai: pip install google-generativeai",
                file=sys.stderr,
            )
            sys.exit(1)

        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(
            [PROMPT_SYSTEM + "\n" + prompt_user],
            generation_config={"temperature": 0.0, "max_output_tokens": 4096},
        )
        print(f"[üìä Gemini - brak szczeg√≥≈Ç√≥w token√≥w]")
        print(f"[üí∞ Gemini - sprawd≈∫ limity w Google AI Studio]")
        return response.text.strip()

    # --- LM Studio ---
    elif ENGINE == "lmstudio":
        LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
        LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_LM", "llama-3.3-70b-instruct"
        )

        try:
            from openai import OpenAI
        except ImportError:
            print("‚ùå Musisz zainstalowaƒá openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie token√≥w
        tokens = resp.usage
        print(
            f"[üìä Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[üí∞ Model lokalny - brak koszt√≥w]")
        return resp.choices[0].message.content.strip()

    # --- Anything LLM ---
    elif ENGINE == "anything":
        ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
        ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_ANY", "llama-3.3-70b-instruct"
        )

        try:
            from openai import OpenAI
        except ImportError:
            print("‚ùå Musisz zainstalowaƒá openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie token√≥w
        tokens = resp.usage
        print(
            f"[üìä Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[üí∞ Model lokalny - brak koszt√≥w]")
        return resp.choices[0].message.content.strip()
    else:
        print(f"‚ùå Nieznany silnik: {ENGINE}", file=sys.stderr)
        sys.exit(1)


def extract_flag(text: str) -> str:
    import re

    m = re.search(r"\{\{FLG:[^}]+\}\}|FLG\{[^}]+\}", text)
    return m.group(0) if m else ""


def main():
    raw = download_text(CENZURA_URL)
    print(f"üîÑ Pobrano tekst ({len(raw)} znak√≥w)")
    print(f"üîÑ Cenzurujƒô u≈ºywajƒÖc {ENGINE}...")

    censored = censor_llm(raw)
    print("=== OCENZUROWANY OUTPUT ===")
    print(censored)
    print("===========================")

    payload = {"task": "CENZURA", "apikey": CENTRALA_API_KEY, "answer": censored}
    try:
        r = requests.post(REPORT_URL, json=payload, timeout=10)
        if r.ok:
            resp_text = r.text.strip()
            flag = extract_flag(resp_text) or extract_flag(censored)
            if flag:
                print(flag)
            else:
                print("Brak flagi w odpowiedzi serwera. Odpowied≈∫:", resp_text)
        else:
            print(f"‚ùå B≈ÇƒÖd HTTP {r.status_code}: {r.text}")
    except Exception as e:
        print(f"‚ùå B≈ÇƒÖd podczas wysy≈Çania danych: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
