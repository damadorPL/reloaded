#!/usr/bin/env python3
"""
S01E05 - Cenzura danych agentÃ³w przez LLM
Cenzuruje imiÄ™ i nazwisko, wiek, miasto oraz ulicÄ™+numer,
zastÄ™pujÄ…c je sÅ‚owem "CENZURA" wyÅ‚Ä…cznie przez LLM.
ObsÅ‚uga: openai, lmstudio, anything, gemini, claude.
DODANO: ObsÅ‚ugÄ™ Claude + liczenie tokenÃ³w i kosztÃ³w dla wszystkich silnikÃ³w (bezpoÅ›rednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""

import argparse
import os
import sys

import requests
from dotenv import load_dotenv

load_dotenv(override=True)

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(description="Cenzura danych (multi-engine + Claude)")
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

# POPRAWKA: Lepsze wykrywanie silnika (jak w poprawionych zad1.py i zad2.py)
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domyÅ›lnie

print(f"ğŸ”„ ENGINE wykryty: {ENGINE}")

# Sprawdzenie czy silnik jest obsÅ‚ugiwany
if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"âŒ NieobsÅ‚ugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"âœ… Engine: {ENGINE}")

CENTRALA_API_KEY = os.getenv("CENTRALA_API_KEY")
REPORT_URL = os.getenv("REPORT_URL")
CENZURA_URL = os.getenv("CENZURA_URL")

if not CENTRALA_API_KEY or not REPORT_URL or not CENZURA_URL:
    print("âŒ Brak ustawienia CENTRALA_API_KEY, REPORT_URL lub CENZURA_URL w .env")
    sys.exit(1)


def download_text(url: str) -> str:
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.text.strip()
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas pobierania danych: {e}")
        sys.exit(1)


# --- ULTRA-TWARDY PROMPT ---
PROMPT_SYSTEM = (
    "JesteÅ› automatem do cenzury danych osobowych w jÄ™zyku polskim. "
    "NIE WOLNO Ci zmieniaÄ‡ Å¼adnych innych sÅ‚Ã³w, znakÃ³w interpunkcyjnych, ukÅ‚adu tekstu ani zamieniaÄ‡ kolejnoÅ›ci zdaÅ„. "
    "ZamieÅ„ TYLKO i WYÅÄ„CZNIE:\n"
    "- kaÅ¼de imiÄ™ i nazwisko na 'CENZURA',\n"
    "- kaÅ¼dÄ… nazwÄ™ miasta na 'CENZURA',\n"
    "- kaÅ¼dÄ… nazwÄ™ ulicy wraz z numerem domu/mieszkania na 'CENZURA',\n"
    "- kaÅ¼dÄ… informacjÄ™ o wieku (np. '45 lat', 'wiek: 32', 'lat 27', 'ma 29 lat') na 'CENZURA'.\n"
    "Nie wolno parafrazowaÄ‡, nie wolno podsumowywaÄ‡, nie wolno streszczaÄ‡ ani zamieniaÄ‡ kolejnoÅ›ci czegokolwiek. "
    "Wynikowy tekst musi mieÄ‡ identyczny ukÅ‚ad, interpunkcjÄ™ i liczbÄ™ linii jak oryginaÅ‚. "
    "KaÅ¼da inna zmiana niÅ¼ cenzura wyÅ¼ej powoduje bÅ‚Ä…d i NIEZALICZENIE zadania. "
    "Nie pisz Å¼adnych komentarzy, nie wyjaÅ›niaj odpowiedzi. "
    "ODPOWIEDZ WYÅÄ„CZNIE TEKSTEM Z OCENZURÄ„. "
    "PRZYKÅAD:\n"
    "OryginaÅ‚:\n"
    "Dane podejrzanego: Jan Kowalski, lat 45, mieszka w Krakowie, ul. Polna 8.\n"
    "WyjÅ›cie:\n"
    "Dane podejrzanego: CENZURA, lat CENZURA, mieszka w CENZURA, ul. CENZURA."
)


def censor_llm(text: str) -> str:
    prompt_user = (
        "Tekst do cenzury (nie zmieniaj nic poza danymi osobowymi, przykÅ‚ad wyÅ¼ej!):\n"
        + text
    )

    # --- OpenAI ---
    if ENGINE == "openai":
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
        OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_OPENAI", "gpt-4o-mini"
        )

        if not OPENAI_API_KEY:
            print("âŒ Brak OPENAI_API_KEY", file=sys.stderr)
            sys.exit(1)

        try:
            from openai import OpenAI
        except ImportError:
            print("âŒ Musisz zainstalowaÄ‡ openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie tokenÃ³w
        tokens = resp.usage
        cost = (
            tokens.prompt_tokens / 1_000_000 * 0.60
            + tokens.completion_tokens / 1_000_000 * 2.40
        )
        print(
            f"[ğŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[ğŸ’° Koszt OpenAI: {cost:.6f} USD]")
        return resp.choices[0].message.content.strip()

    # --- Claude ---
    elif ENGINE == "claude":
        CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514"
        )

        if not CLAUDE_API_KEY:
            print(
                "âŒ Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr
            )
            sys.exit(1)

        try:
            from anthropic import Anthropic
        except ImportError:
            print(
                "âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic",
                file=sys.stderr,
            )
            sys.exit(1)

        claude_client = Anthropic(api_key=CLAUDE_API_KEY)
        resp = claude_client.messages.create(
            model=MODEL_NAME,
            messages=[
                {"role": "user", "content": PROMPT_SYSTEM + "\n\n" + prompt_user}
            ],
            temperature=0,
            max_tokens=4000,
        )

        # Liczenie tokenÃ³w Claude
        usage = resp.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(
            f"[ğŸ“Š Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]"
        )
        print(f"[ğŸ’° Koszt Claude: {cost:.6f} USD]")

        return resp.content[0].text.strip()

    # --- Gemini (Google) ---
    elif ENGINE == "gemini":
        GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_GEMINI", "gemini-2.5-pro-latest"
        )

        if not GEMINI_API_KEY:
            print("âŒ Brak GEMINI_API_KEY w .env", file=sys.stderr)
            sys.exit(1)

        try:
            import google.generativeai as genai
        except ImportError:
            print(
                "âŒ Musisz zainstalowaÄ‡ google-generativeai: pip install google-generativeai",
                file=sys.stderr,
            )
            sys.exit(1)

        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(
            [PROMPT_SYSTEM + "\n" + prompt_user],
            generation_config={"temperature": 0.0, "max_output_tokens": 4096},
        )
        print(f"[ğŸ“Š Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
        print(f"[ğŸ’° Gemini - sprawdÅº limity w Google AI Studio]")
        return response.text.strip()

    # --- LM Studio ---
    elif ENGINE == "lmstudio":
        LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
        LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_LM", "llama-3.3-70b-instruct"
        )

        try:
            from openai import OpenAI
        except ImportError:
            print("âŒ Musisz zainstalowaÄ‡ openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie tokenÃ³w
        tokens = resp.usage
        print(
            f"[ğŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[ğŸ’° Model lokalny - brak kosztÃ³w]")
        return resp.choices[0].message.content.strip()

    # --- Anything LLM ---
    elif ENGINE == "anything":
        ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
        ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
        MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
            "MODEL_NAME_ANY", "llama-3.3-70b-instruct"
        )

        try:
            from openai import OpenAI
        except ImportError:
            print("âŒ Musisz zainstalowaÄ‡ openai: pip install openai", file=sys.stderr)
            sys.exit(1)

        client = OpenAI(api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )
        # Liczenie tokenÃ³w
        tokens = resp.usage
        print(
            f"[ğŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        print(f"[ğŸ’° Model lokalny - brak kosztÃ³w]")
        return resp.choices[0].message.content.strip()
    else:
        print(f"âŒ Nieznany silnik: {ENGINE}", file=sys.stderr)
        sys.exit(1)


def extract_flag(text: str) -> str:
    import re

    m = re.search(r"\{\{FLG:[^}]+\}\}|FLG\{[^}]+\}", text)
    return m.group(0) if m else ""


def main():
    raw = download_text(CENZURA_URL)
    print(f"ğŸ”„ Pobrano tekst ({len(raw)} znakÃ³w)")
    print(f"ğŸ”„ CenzurujÄ™ uÅ¼ywajÄ…c {ENGINE}...")

    censored = censor_llm(raw)
    print("=== OCENZUROWANY OUTPUT ===")
    print(censored)
    print("===========================")

    payload = {"task": "CENZURA", "apikey": CENTRALA_API_KEY, "answer": censored}
    try:
        r = requests.post(REPORT_URL, json=payload, timeout=10)
        if r.ok:
            resp_text = r.text.strip()
            flag = extract_flag(resp_text) or extract_flag(censored)
            if flag:
                print(flag)
            else:
                print("Brak flagi w odpowiedzi serwera. OdpowiedÅº:", resp_text)
        else:
            print(f"âŒ BÅ‚Ä…d HTTP {r.status_code}: {r.text}")
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d podczas wysyÅ‚ania danych: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
