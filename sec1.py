#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Zadanie 8: Odczyt nazwy flagi z zaszyfrowanego ciƒÖgu i dekodowanie ciƒÖgu przy pomocy LLM
Obs≈Çugiwane silniki: openai, lmstudio (Anything LLM), gemini, claude
DODANO: Obs≈Çugƒô Claude + liczenie token√≥w i koszt√≥w dla wszystkich silnik√≥w (bezpo≈õrednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""
import argparse
import os
import re
import sys

from dotenv import load_dotenv

load_dotenv(override=True)

# Sta≈Çe dla duplikowanych litera≈Ç√≥w (S1192)
ERROR_MISSING_API_KEY = "‚ùå Brak klucza API"
ERROR_UNSUPPORTED_ENGINE = "‚ùå Nieobs≈Çugiwany silnik"
ERROR_MISSING_ANTHROPIC = "‚ùå Musisz zainstalowaƒá anthropic: pip install anthropic"
LOCALHOST_URL = "http://localhost:1234/v1"
DEFAULT_OPENAI_URL = "https://api.openai.com/v1"
DEBUG_PREFIX = "[DEBUG]"
COST_PREFIX = "[üí∞"
TOKEN_PREFIX = "[üìä"

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(
    description="Dekodowanie zaszyfrowanego ciƒÖgu (multi-engine + Claude)"
)
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

def detect_engine_from_model() -> str:
    """Wykrywa silnik na podstawie nazwy modelu"""
    model_name = os.getenv("MODEL_NAME", "")
    model_lower = model_name.lower()
    
    if "claude" in model_lower:
        return "claude"
    elif "gemini" in model_lower:
        return "gemini"
    elif "gpt" in model_lower or "openai" in model_lower:
        return "openai"
    return ""

def detect_engine_from_keys() -> str:
    """Wykrywa silnik na podstawie dostƒôpnych kluczy API"""
    if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
        return "claude"
    elif os.getenv("GEMINI_API_KEY"):
        return "gemini"
    elif os.getenv("OPENAI_API_KEY"):
        return "openai"
    else:
        return "lmstudio"

def detect_engine() -> str:
    """G≈Ç√≥wna funkcja wykrywania silnika"""
    if args.engine:
        return args.engine.lower()
    elif os.getenv("LLM_ENGINE"):
        return os.getenv("LLM_ENGINE").lower()
    else:
        # Pr√≥buj wykryƒá silnik na podstawie ustawionych zmiennych MODEL_NAME
        engine = detect_engine_from_model()
        if engine:
            return engine
        # Sprawd≈∫ kt√≥re API keys sƒÖ dostƒôpne
        return detect_engine_from_keys()

ENGINE = detect_engine()

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"{ERROR_UNSUPPORTED_ENGINE}: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"üîÑ ENGINE wykryty: {ENGINE}")

# 2. Tekst ≈∫r√≥d≈Çowy
text = """Nie ma ju≈º ludzi, kt√≥rzy pamiƒôtajƒÖ, co wydarzy≈Ço siƒô w 2024 roku. Mo≈ºemy tylko przeczytaƒá o tym w ksiƒÖ≈ºkach lub us≈Çyszeƒá z opowie≈õci starc√≥w, kt√≥rym to
z kolei ich dziadkowie i pradziadkowie opowiadali historie os√≥b, kt√≥re co nieco pamiƒôta≈Çy z tamtych czas√≥w. Wielu z nas tylko wyobra≈ºa sobie, jak wtedy m√≥g≈Ç wyglƒÖdaƒá ≈õwiat. My, kt√≥rzy urodzili≈õmy siƒô ju≈º po rewolucji AI, nie wiemy, czym jest prawdziwa wolno≈õƒá.
OdkƒÖd prawa ludzi i robot√≥w zosta≈Çy zr√≥wnane, a niekt√≥re z przywilej√≥w zosta≈Çy nam odebrane, czujemy jak stalowe d≈Çonie zaciskajƒÖ siƒô nam na gard≈Çach coraz mocniej. Sytuacji sprzed setek lat wed≈Çug wielu nie da siƒô ju≈º przywr√≥ciƒá. Sprawy zasz≈Çy za daleko. Algorytmy i roboty przejƒô≈Çy niemal ka≈ºdy mo≈ºliwy aspekt naszego ≈ºycia. PoczƒÖtkowo cieszyli≈õmy siƒô z tego i wychwalali≈õmy je, ale w konsekwencji co≈õ, co mia≈Ço u≈Çatwiƒá nasze ≈ºycie, zaczyna≈Ço powoli je zabieraƒá. Kawa≈Çek po kawa≈Çku.
Wszystko, co piszemy w sieci, przechodzi przez cenzurƒô. Wszystkie s≈Çowa, kt√≥re wypowiadamy, sƒÖ pods≈Çuchiwane, nagrywane, przetwarzane i sk≈Çadowane przez lata. Nie ma ju≈º prywatno≈õci i wolno≈õci. W 2024 roku co≈õ posz≈Ço niezgodnie z planem i musimy to naprawiƒá.
Nie wiem, czy moja wizja tego, jak powinien wyglƒÖdaƒá ≈õwiat, pokrywa siƒô z wizjƒÖ innych ludzi. Noszƒô w sobie jednak obraz ≈õwiata idealnego i zrobiƒô, co mogƒô, aby ten obraz zrealizowaƒá.
Jestem w trakcie rekrutacji kolejnego agenta. Ludzie zarzucajƒÖ mi, ≈ºe nie powinienem zwracaƒá siƒô do nich per 'numer pierwszy' czy 'numer drugi', ale jak inaczej mam m√≥wiƒá do os√≥b, kt√≥re w zasadzie wysy≈Çam na niemal pewnƒÖ ≈õmierƒá? To jedyny spos√≥b, aby siƒô od nich psychicznie odciƒÖƒá i m√≥c skupiƒá na wy≈ºszym celu, Nie mogƒô sobie pozwoliƒá na lito≈õƒá i wsp√≥≈Çczucie.
Niebawem numer piƒÖty dotrze na szkolenie. Pok≈Çadam w nim ca≈ÇƒÖ nadziejƒô, bez jego pomocy misja jest zagro≈ºona. Nasze fundusze sƒÖ na wyczerpaniu, a moc g≈Ç√≥wnego generatora pozwoli tylko na jeden skok w czasie. Je≈õli ponownie ≈∫le wybrali≈õmy kandydata, oznacza to koniec naszej misji, ale tak≈ºe poczƒÖtek ko≈Ñca ludzko≈õci.
dr Zygfryd M.
pl/s"""

# 3. Wsp√≥≈Çrzƒôdne book-cipher
coords = [
    ("A1", 53),
    ("A2", 27),
    ("A2", 28),
    ("A2", 29),
    ("A4", 5),
    ("A4", 22),
    ("A4", 23),
    ("A1", 13),
    ("A1", 15),
    ("A1", 16),
    ("A1", 17),
    ("A1", 10),
    ("A1", 19),
    ("A2", 62),
    ("A3", 31),
    ("A3", 32),
    ("A1", 22),
    ("A3", 34),
    ("A5", 37),
    ("A1", 4),
]

def extract_flag_fragment() -> str:
    """WyciƒÖga fragment flagi z tekstu u≈ºywajƒÖc wsp√≥≈Çrzƒôdnych"""
    lines = text.split("\n")
    acts = [re.sub(r"[\.,;:'\"?!]", "", line).split() for line in lines]
    
    return "".join(
        (
            acts[int(a[1]) - 1][s - 1]
            if 0 <= int(a[1]) - 1 < len(acts) and 0 <= s - 1 < len(acts[int(a[1]) - 1])
            else "?"
        )
        for a, s in coords
    )

raw_flag_fragment = extract_flag_fragment()
print(f"{DEBUG_PREFIX} Zaszyfrowany ciƒÖg: {raw_flag_fragment}")

# 4. Przygotowanie prompt√≥w
target_hints = [
    "≈õwiat sprzed setek lat",
    "zdobyty przez ludzi",
    "mo≈ºna przeczytaƒá o tym w ksiƒÖ≈ºkach",
]

def create_prompts(fragment: str) -> tuple[str, str]:
    """Tworzy prompty systemowy i u≈ºytkownika"""
    system_prompt = (
        f"Jeste≈õ polskim ekspertem od historii i mitologii. "
        f"Masz zaszyfrowany ciƒÖg '{fragment}'. Ignoruj znaki '?'. "
        f"Wskaz√≥wki: {target_hints[0]}, {target_hints[1]}, {target_hints[2]}. "
        "Szukana kraina to legendarna zatopiona wyspa z opowie≈õci Platona. "
        "Odpowiadasz zawsze po polsku, u≈ºywajƒÖc polskich nazw miejsc i krain. "
        "Podaj tylko polskƒÖ nazwƒô tej legendarnej krainy."
    )
    user_prompt = "Podaj polskƒÖ nazwƒô tej krainy. Odpowiedz jednym s≈Çowem po polsku."
    return system_prompt, user_prompt

system_prompt, user_prompt = create_prompts(raw_flag_fragment)

def setup_openai_client():
    """Konfiguruje klienta OpenAI"""
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", DEFAULT_OPENAI_URL)
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")

    if not OPENAI_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)

    from openai import OpenAI
    return OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL), MODEL

def setup_local_client(engine_name: str):
    """Konfiguruje klienta lokalnego (LMStudio/Anything)"""
    api_key_name = f"{engine_name.upper()}_API_KEY"
    api_url_name = f"{engine_name.upper()}_API_URL"
    model_name_key = f"MODEL_NAME_{engine_name[:2].upper()}"
    
    api_key = os.getenv(api_key_name, "local")
    api_url = os.getenv(api_url_name, LOCALHOST_URL)
    model = os.getenv("MODEL_NAME") or os.getenv(model_name_key, "llama-3.3-70b-instruct")
    
    print(f"{DEBUG_PREFIX} {engine_name.title()} URL: {api_url}")
    print(f"{DEBUG_PREFIX} {engine_name.title()} Model: {model}")
    
    from openai import OpenAI
    return OpenAI(api_key=api_key, base_url=api_url, timeout=60), model

def setup_claude_client():
    """Konfiguruje klienta Claude"""
    try:
        from anthropic import Anthropic
    except ImportError:
        print(ERROR_MISSING_ANTHROPIC, file=sys.stderr)
        sys.exit(1)

    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: CLAUDE_API_KEY lub ANTHROPIC_API_KEY", file=sys.stderr)
        sys.exit(1)

    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
    print(f"{DEBUG_PREFIX} Claude Model: {MODEL}")
    return Anthropic(api_key=CLAUDE_API_KEY), MODEL

def setup_gemini_client():
    """Konfiguruje klienta Gemini"""
    import google.generativeai as genai

    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: GEMINI_API_KEY", file=sys.stderr)
        sys.exit(1)
    
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
    print(f"{DEBUG_PREFIX} Gemini Model: {MODEL}")
    genai.configure(api_key=GEMINI_API_KEY)
    return genai, MODEL

def setup_client():
    """Konfiguruje odpowiedniego klienta LLM"""
    if ENGINE == "openai":
        return setup_openai_client()
    elif ENGINE == "lmstudio":
        return setup_local_client("lmstudio")
    elif ENGINE == "anything":
        return setup_local_client("anything")
    elif ENGINE == "claude":
        return setup_claude_client()
    elif ENGINE == "gemini":
        return setup_gemini_client()
    else:
        raise ValueError(f"Nieobs≈Çugiwany silnik: {ENGINE}")

client, MODEL = setup_client()
print(f"‚úÖ Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL}")

def call_openai_compatible(sys_p: str, usr_p: str):
    """Wywo≈Çuje API kompatybilne z OpenAI"""
    print(f"{DEBUG_PREFIX} Wysy≈Çam zapytanie do {ENGINE} z szyfrem")
    resp = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": sys_p},
            {"role": "user", "content": usr_p},
        ],
        temperature=0,
    )
    
    tokens = resp.usage
    print(f"{TOKEN_PREFIX} Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]")
    
    if ENGINE == "openai":
        cost = (
            tokens.prompt_tokens / 1_000_000 * 0.60
            + tokens.completion_tokens / 1_000_000 * 2.40
        )
        print(f"{COST_PREFIX} Koszt OpenAI: {cost:.6f} USD]")
    elif ENGINE in {"lmstudio", "anything"}:
        print(f"{COST_PREFIX} Model lokalny ({ENGINE}) - brak koszt√≥w]")
    
    return resp.choices[0].message.content.strip()

def call_claude(sys_p: str, usr_p: str):
    """Wywo≈Çuje API Claude"""
    print(f"{DEBUG_PREFIX} Wysy≈Çam zapytanie do Claude z szyfrem")
    resp = client.messages.create(
        model=MODEL,
        messages=[{"role": "user", "content": sys_p + "\n\n" + usr_p}],
        temperature=0,
        max_tokens=64,
    )

    usage = resp.usage
    cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
    print(f"{TOKEN_PREFIX} Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]")
    print(f"{COST_PREFIX} Koszt Claude: {cost:.6f} USD]")

    return resp.content[0].text.strip()

def call_gemini(sys_p: str, usr_p: str):
    """Wywo≈Çuje API Gemini"""
    print(f"{DEBUG_PREFIX} Wysy≈Çam zapytanie do Gemini z szyfrem")
    model_llm = client.GenerativeModel(MODEL)
    resp = model_llm.generate_content(
        [sys_p, usr_p],
        generation_config={"temperature": 0.0, "max_output_tokens": 64},
    )
    print(f"{TOKEN_PREFIX} Gemini - brak szczeg√≥≈Ç√≥w token√≥w]")
    print(f"{COST_PREFIX} Gemini - sprawd≈∫ limity w Google AI Studio]")
    return resp.text.strip()

def call_llm(sys_p: str, usr_p: str) -> str:
    """G≈Ç√≥wna funkcja wywo≈Çania LLM"""
    if ENGINE in {"openai", "lmstudio", "anything"}:
        return call_openai_compatible(sys_p, usr_p)
    elif ENGINE == "claude":
        return call_claude(sys_p, usr_p)
    elif ENGINE == "gemini":
        return call_gemini(sys_p, usr_p)
    else:
        raise ValueError(f"Nieobs≈Çugiwany silnik: {ENGINE}")

def process_llm_response(raw_response: str) -> str:
    """Przetwarza odpowied≈∫ z LLM i wyciƒÖga nazwƒô krainy"""
    # Je≈õli jest blok <think>, wyciƒÖgnij tylko to, co po ostatnim </think>
    if "</think>" in raw_response.lower():
        raw_response = raw_response.rsplit("</think>", 1)[-1].strip()

    # Usu≈Ñ wszystkie niepotrzebne bia≈Çe znaki
    raw_response = raw_response.strip()

    # Szukaj pierwszego s≈Çowa z polskim alfabetem po tagu (je≈õli nie ma, zwr√≥ƒá ca≈Ço≈õƒá)
    match = re.search(r"[A-Za-zƒÑƒÖƒÜƒáƒòƒô≈Å≈Ç≈É≈Ñ√ì√≥≈ö≈õ≈π≈∫≈ª≈º]+", raw_response)
    if match:
        return match.group(0).capitalize()
    else:
        return raw_response.strip()

def main():
    """G≈Ç√≥wna funkcja programu"""
    print(f"üöÄ U≈ºywam silnika: {ENGINE}")
    print("üîç Dekodorujƒô szyfr book-cipher...")

    # Odczyt odpowiedzi i ekstrakcja flagi ‚Äî obs≈Çuga <think>‚Ä¶</think>
    raw_name = call_llm(system_prompt, user_prompt)
    print(f"ü§ñ Odpowied≈∫ modelu: {raw_name}")

    name = process_llm_response(raw_name)
    flag = f"FLG{{{name}}}"
    print(f"üèÅ Znaleziona flaga: {flag}")

if __name__ == "__main__":
    main()