#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Zadanie 8: Odczyt nazwy flagi z zaszyfrowanego ciÄ…gu i dekodowanie ciÄ…gu przy pomocy LLM
ObsÅ‚ugiwane silniki: openai, lmstudio (Anything LLM), gemini, claude
DODANO: ObsÅ‚ugÄ™ Claude + liczenie tokenÃ³w i kosztÃ³w dla wszystkich silnikÃ³w (bezpoÅ›rednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""
import argparse
import os
import re
import sys

from dotenv import load_dotenv

load_dotenv(override=True)

# StaÅ‚e dla duplikowanych literaÅ‚Ã³w (S1192)
ERROR_MISSING_API_KEY = "âŒ Brak klucza API"
ERROR_UNSUPPORTED_ENGINE = "âŒ NieobsÅ‚ugiwany silnik"
ERROR_MISSING_ANTHROPIC = "âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic"
LOCALHOST_URL = "http://localhost:1234/v1"
DEFAULT_OPENAI_URL = "https://api.openai.com/v1"
DEBUG_PREFIX = "[DEBUG]"
COST_PREFIX = "[ğŸ’°"
TOKEN_PREFIX = "[ğŸ“Š"

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(
    description="Dekodowanie zaszyfrowanego ciÄ…gu (multi-engine + Claude)"
)
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

def detect_engine_from_model() -> str:
    """Wykrywa silnik na podstawie nazwy modelu"""
    model_name = os.getenv("MODEL_NAME", "")
    model_lower = model_name.lower()
    
    if "claude" in model_lower:
        return "claude"
    elif "gemini" in model_lower:
        return "gemini"
    elif "gpt" in model_lower or "openai" in model_lower:
        return "openai"
    return ""

def detect_engine_from_keys() -> str:
    """Wykrywa silnik na podstawie dostÄ™pnych kluczy API"""
    if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
        return "claude"
    elif os.getenv("GEMINI_API_KEY"):
        return "gemini"
    elif os.getenv("OPENAI_API_KEY"):
        return "openai"
    else:
        return "lmstudio"

def detect_engine() -> str:
    """GÅ‚Ã³wna funkcja wykrywania silnika"""
    if args.engine:
        return args.engine.lower()
    elif os.getenv("LLM_ENGINE"):
        return os.getenv("LLM_ENGINE").lower()
    else:
        # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
        engine = detect_engine_from_model()
        if engine:
            return engine
        # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
        return detect_engine_from_keys()

ENGINE = detect_engine()

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"{ERROR_UNSUPPORTED_ENGINE}: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"ğŸ”„ ENGINE wykryty: {ENGINE}")

# 2. Tekst ÅºrÃ³dÅ‚owy
text = """Nie ma juÅ¼ ludzi, ktÃ³rzy pamiÄ™tajÄ…, co wydarzyÅ‚o siÄ™ w 2024 roku. MoÅ¼emy tylko przeczytaÄ‡ o tym w ksiÄ…Å¼kach lub usÅ‚yszeÄ‡ z opowieÅ›ci starcÃ³w, ktÃ³rym to
z kolei ich dziadkowie i pradziadkowie opowiadali historie osÃ³b, ktÃ³re co nieco pamiÄ™taÅ‚y z tamtych czasÃ³w. Wielu z nas tylko wyobraÅ¼a sobie, jak wtedy mÃ³gÅ‚ wyglÄ…daÄ‡ Å›wiat. My, ktÃ³rzy urodziliÅ›my siÄ™ juÅ¼ po rewolucji AI, nie wiemy, czym jest prawdziwa wolnoÅ›Ä‡.
OdkÄ…d prawa ludzi i robotÃ³w zostaÅ‚y zrÃ³wnane, a niektÃ³re z przywilejÃ³w zostaÅ‚y nam odebrane, czujemy jak stalowe dÅ‚onie zaciskajÄ… siÄ™ nam na gardÅ‚ach coraz mocniej. Sytuacji sprzed setek lat wedÅ‚ug wielu nie da siÄ™ juÅ¼ przywrÃ³ciÄ‡. Sprawy zaszÅ‚y za daleko. Algorytmy i roboty przejÄ™Å‚y niemal kaÅ¼dy moÅ¼liwy aspekt naszego Å¼ycia. PoczÄ…tkowo cieszyliÅ›my siÄ™ z tego i wychwalaliÅ›my je, ale w konsekwencji coÅ›, co miaÅ‚o uÅ‚atwiÄ‡ nasze Å¼ycie, zaczynaÅ‚o powoli je zabieraÄ‡. KawaÅ‚ek po kawaÅ‚ku.
Wszystko, co piszemy w sieci, przechodzi przez cenzurÄ™. Wszystkie sÅ‚owa, ktÃ³re wypowiadamy, sÄ… podsÅ‚uchiwane, nagrywane, przetwarzane i skÅ‚adowane przez lata. Nie ma juÅ¼ prywatnoÅ›ci i wolnoÅ›ci. W 2024 roku coÅ› poszÅ‚o niezgodnie z planem i musimy to naprawiÄ‡.
Nie wiem, czy moja wizja tego, jak powinien wyglÄ…daÄ‡ Å›wiat, pokrywa siÄ™ z wizjÄ… innych ludzi. NoszÄ™ w sobie jednak obraz Å›wiata idealnego i zrobiÄ™, co mogÄ™, aby ten obraz zrealizowaÄ‡.
Jestem w trakcie rekrutacji kolejnego agenta. Ludzie zarzucajÄ… mi, Å¼e nie powinienem zwracaÄ‡ siÄ™ do nich per 'numer pierwszy' czy 'numer drugi', ale jak inaczej mam mÃ³wiÄ‡ do osÃ³b, ktÃ³re w zasadzie wysyÅ‚am na niemal pewnÄ… Å›mierÄ‡? To jedyny sposÃ³b, aby siÄ™ od nich psychicznie odciÄ…Ä‡ i mÃ³c skupiÄ‡ na wyÅ¼szym celu, Nie mogÄ™ sobie pozwoliÄ‡ na litoÅ›Ä‡ i wspÃ³Å‚czucie.
Niebawem numer piÄ…ty dotrze na szkolenie. PokÅ‚adam w nim caÅ‚Ä… nadziejÄ™, bez jego pomocy misja jest zagroÅ¼ona. Nasze fundusze sÄ… na wyczerpaniu, a moc gÅ‚Ã³wnego generatora pozwoli tylko na jeden skok w czasie. JeÅ›li ponownie Åºle wybraliÅ›my kandydata, oznacza to koniec naszej misji, ale takÅ¼e poczÄ…tek koÅ„ca ludzkoÅ›ci.
dr Zygfryd M.
pl/s"""

# 3. WspÃ³Å‚rzÄ™dne book-cipher
coords = [
    ("A1", 53),
    ("A2", 27),
    ("A2", 28),
    ("A2", 29),
    ("A4", 5),
    ("A4", 22),
    ("A4", 23),
    ("A1", 13),
    ("A1", 15),
    ("A1", 16),
    ("A1", 17),
    ("A1", 10),
    ("A1", 19),
    ("A2", 62),
    ("A3", 31),
    ("A3", 32),
    ("A1", 22),
    ("A3", 34),
    ("A5", 37),
    ("A1", 4),
]

def extract_flag_fragment() -> str:
    """WyciÄ…ga fragment flagi z tekstu uÅ¼ywajÄ…c wspÃ³Å‚rzÄ™dnych"""
    lines = text.split("\n")
    acts = [re.sub(r"[\.,;:'\"?!]", "", line).split() for line in lines]
    
    return "".join(
        (
            acts[int(a[1]) - 1][s - 1]
            if 0 <= int(a[1]) - 1 < len(acts) and 0 <= s - 1 < len(acts[int(a[1]) - 1])
            else "?"
        )
        for a, s in coords
    )

raw_flag_fragment = extract_flag_fragment()
print(f"{DEBUG_PREFIX} Zaszyfrowany ciÄ…g: {raw_flag_fragment}")

# 4. Przygotowanie promptÃ³w
target_hints = [
    "Å›wiat sprzed setek lat",
    "zdobyty przez ludzi",
    "moÅ¼na przeczytaÄ‡ o tym w ksiÄ…Å¼kach",
]

def create_prompts(fragment: str) -> tuple[str, str]:
    """Tworzy prompty systemowy i uÅ¼ytkownika"""
    system_prompt = (
        f"JesteÅ› polskim ekspertem od historii i mitologii. "
        f"Masz zaszyfrowany ciÄ…g '{fragment}'. Ignoruj znaki '?'. "
        f"WskazÃ³wki: {target_hints[0]}, {target_hints[1]}, {target_hints[2]}. "
        "Szukana kraina to legendarna zatopiona wyspa z opowieÅ›ci Platona. "
        "Odpowiadasz zawsze po polsku, uÅ¼ywajÄ…c polskich nazw miejsc i krain. "
        "Podaj tylko polskÄ… nazwÄ™ tej legendarnej krainy."
    )
    user_prompt = "Podaj polskÄ… nazwÄ™ tej krainy. Odpowiedz jednym sÅ‚owem po polsku."
    return system_prompt, user_prompt

system_prompt, user_prompt = create_prompts(raw_flag_fragment)

def setup_openai_client():
    """Konfiguruje klienta OpenAI"""
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", DEFAULT_OPENAI_URL)
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")

    if not OPENAI_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)

    from openai import OpenAI
    return OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL), MODEL

def setup_local_client(engine_name: str):
    """Konfiguruje klienta lokalnego (LMStudio/Anything)"""
    api_key_name = f"{engine_name.upper()}_API_KEY"
    api_url_name = f"{engine_name.upper()}_API_URL"
    model_name_key = f"MODEL_NAME_{engine_name[:2].upper()}"
    
    api_key = os.getenv(api_key_name, "local")
    api_url = os.getenv(api_url_name, LOCALHOST_URL)
    model = os.getenv("MODEL_NAME") or os.getenv(model_name_key, "llama-3.3-70b-instruct")
    
    print(f"{DEBUG_PREFIX} {engine_name.title()} URL: {api_url}")
    print(f"{DEBUG_PREFIX} {engine_name.title()} Model: {model}")
    
    from openai import OpenAI
    return OpenAI(api_key=api_key, base_url=api_url, timeout=60), model

def setup_claude_client():
    """Konfiguruje klienta Claude"""
    try:
        from anthropic import Anthropic
    except ImportError:
        print(ERROR_MISSING_ANTHROPIC, file=sys.stderr)
        sys.exit(1)

    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: CLAUDE_API_KEY lub ANTHROPIC_API_KEY", file=sys.stderr)
        sys.exit(1)

    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
    print(f"{DEBUG_PREFIX} Claude Model: {MODEL}")
    return Anthropic(api_key=CLAUDE_API_KEY), MODEL

def setup_gemini_client():
    """Konfiguruje klienta Gemini"""
    import google.generativeai as genai

    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print(f"{ERROR_MISSING_API_KEY}: GEMINI_API_KEY", file=sys.stderr)
        sys.exit(1)
    
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
    print(f"{DEBUG_PREFIX} Gemini Model: {MODEL}")
    genai.configure(api_key=GEMINI_API_KEY)
    return genai, MODEL

def setup_client():
    """Konfiguruje odpowiedniego klienta LLM"""
    if ENGINE == "openai":
        return setup_openai_client()
    elif ENGINE == "lmstudio":
        return setup_local_client("lmstudio")
    elif ENGINE == "anything":
        return setup_local_client("anything")
    elif ENGINE == "claude":
        return setup_claude_client()
    elif ENGINE == "gemini":
        return setup_gemini_client()
    else:
        raise ValueError(f"NieobsÅ‚ugiwany silnik: {ENGINE}")

client, MODEL = setup_client()
print(f"âœ… Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL}")

def call_openai_compatible(sys_p: str, usr_p: str):
    """WywoÅ‚uje API kompatybilne z OpenAI"""
    print(f"{DEBUG_PREFIX} WysyÅ‚am zapytanie do {ENGINE} z szyfrem")
    resp = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "system", "content": sys_p},
            {"role": "user", "content": usr_p},
        ],
        temperature=0,
    )
    
    tokens = resp.usage
    print(f"{TOKEN_PREFIX} Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]")
    
    if ENGINE == "openai":
        cost = (
            tokens.prompt_tokens / 1_000_000 * 0.60
            + tokens.completion_tokens / 1_000_000 * 2.40
        )
        print(f"{COST_PREFIX} Koszt OpenAI: {cost:.6f} USD]")
    elif ENGINE in {"lmstudio", "anything"}:
        print(f"{COST_PREFIX} Model lokalny ({ENGINE}) - brak kosztÃ³w]")
    
    return resp.choices[0].message.content.strip()

def call_claude(sys_p: str, usr_p: str):
    """WywoÅ‚uje API Claude"""
    print(f"{DEBUG_PREFIX} WysyÅ‚am zapytanie do Claude z szyfrem")
    resp = client.messages.create(
        model=MODEL,
        messages=[{"role": "user", "content": sys_p + "\n\n" + usr_p}],
        temperature=0,
        max_tokens=64,
    )

    usage = resp.usage
    cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
    print(f"{TOKEN_PREFIX} Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]")
    print(f"{COST_PREFIX} Koszt Claude: {cost:.6f} USD]")

    return resp.content[0].text.strip()

def call_gemini(sys_p: str, usr_p: str):
    """WywoÅ‚uje API Gemini"""
    print(f"{DEBUG_PREFIX} WysyÅ‚am zapytanie do Gemini z szyfrem")
    model_llm = client.GenerativeModel(MODEL)
    resp = model_llm.generate_content(
        [sys_p, usr_p],
        generation_config={"temperature": 0.0, "max_output_tokens": 64},
    )
    print(f"{TOKEN_PREFIX} Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
    print(f"{COST_PREFIX} Gemini - sprawdÅº limity w Google AI Studio]")
    return resp.text.strip()

def call_llm(sys_p: str, usr_p: str) -> str:
    """GÅ‚Ã³wna funkcja wywoÅ‚ania LLM"""
    if ENGINE in {"openai", "lmstudio", "anything"}:
        return call_openai_compatible(sys_p, usr_p)
    elif ENGINE == "claude":
        return call_claude(sys_p, usr_p)
    elif ENGINE == "gemini":
        return call_gemini(sys_p, usr_p)
    else:
        raise ValueError(f"NieobsÅ‚ugiwany silnik: {ENGINE}")

def process_llm_response(raw_response: str) -> str:
    """Przetwarza odpowiedÅº z LLM i wyciÄ…ga nazwÄ™ krainy"""
    # JeÅ›li jest blok <think>, wyciÄ…gnij tylko to, co po ostatnim </think>
    if "</think>" in raw_response.lower():
        raw_response = raw_response.rsplit("</think>", 1)[-1].strip()

    # UsuÅ„ wszystkie niepotrzebne biaÅ‚e znaki
    raw_response = raw_response.strip()

    # Szukaj pierwszego sÅ‚owa z polskim alfabetem po tagu (jeÅ›li nie ma, zwrÃ³Ä‡ caÅ‚oÅ›Ä‡)
    match = re.search(r"[A-Za-zÄ„Ä…Ä†Ä‡Ä˜Ä™ÅÅ‚ÅƒÅ„Ã“Ã³ÅšÅ›Å¹ÅºÅ»Å¼]+", raw_response)
    if match:
        return match.group(0).capitalize()
    else:
        return raw_response.strip()

def main():
    """GÅ‚Ã³wna funkcja programu"""
    print(f"ğŸš€ UÅ¼ywam silnika: {ENGINE}")
    print("ğŸ” DekodorujÄ™ szyfr book-cipher...")

    # Odczyt odpowiedzi i ekstrakcja flagi â€” obsÅ‚uga <think>â€¦</think>
    raw_name = call_llm(system_prompt, user_prompt)
    print(f"ğŸ¤– OdpowiedÅº modelu: {raw_name}")

    name = process_llm_response(raw_name)
    flag = f"FLG{{{name}}}"
    print(f"ğŸ Znaleziona flaga: {flag}")

if __name__ == "__main__":
    main()