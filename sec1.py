#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Zadanie 8: Odczyt nazwy flagi z zaszyfrowanego ciÄ…gu i dekodowanie ciÄ…gu przy pomocy LLM
ObsÅ‚ugiwane silniki: openai, lmstudio (Anything LLM), gemini, claude
DODANO: ObsÅ‚ugÄ™ Claude + liczenie tokenÃ³w i kosztÃ³w dla wszystkich silnikÃ³w (bezpoÅ›rednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""
import argparse
import os
import re
import sys
from dotenv import load_dotenv

load_dotenv(override=True)

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(description="Dekodowanie zaszyfrowanego ciÄ…gu (multi-engine + Claude)")
parser.add_argument("--engine", choices=["openai", "lmstudio", "anything", "gemini", "claude"],
                    help="LLM backend to use")
args = parser.parse_args()

# POPRAWKA: Lepsze wykrywanie silnika (jak w poprawionych zad1.py-zad7.py)
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domyÅ›lnie

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"âŒ NieobsÅ‚ugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"ğŸ”„ ENGINE wykryty: {ENGINE}")

# 2. Tekst ÅºrÃ³dÅ‚owy
text = """Nie ma juÅ¼ ludzi, ktÃ³rzy pamiÄ™tajÄ…, co wydarzyÅ‚o siÄ™ w 2024 roku. MoÅ¼emy tylko przeczytaÄ‡ o tym w ksiÄ…Å¼kach lub usÅ‚yszeÄ‡ z opowieÅ›ci starcÃ³w, ktÃ³rym to
z kolei ich dziadkowie i pradziadkowie opowiadali historie osÃ³b, ktÃ³re co nieco pamiÄ™taÅ‚y z tamtych czasÃ³w. Wielu z nas tylko wyobraÅ¼a sobie, jak wtedy mÃ³gÅ‚ wyglÄ…daÄ‡ Å›wiat. My, ktÃ³rzy urodziliÅ›my siÄ™ juÅ¼ po rewolucji AI, nie wiemy, czym jest prawdziwa wolnoÅ›Ä‡.
OdkÄ…d prawa ludzi i robotÃ³w zostaÅ‚y zrÃ³wnane, a niektÃ³re z przywilejÃ³w zostaÅ‚y nam odebrane, czujemy jak stalowe dÅ‚onie zaciskajÄ… siÄ™ nam na gardÅ‚ach coraz mocniej. Sytuacji sprzed setek lat wedÅ‚ug wielu nie da siÄ™ juÅ¼ przywrÃ³ciÄ‡. Sprawy zaszÅ‚y za daleko. Algorytmy i roboty przejÄ™Å‚y niemal kaÅ¼dy moÅ¼liwy aspekt naszego Å¼ycia. PoczÄ…tkowo cieszyliÅ›my siÄ™ z tego i wychwalaliÅ›my je, ale w konsekwencji coÅ›, co miaÅ‚o uÅ‚atwiÄ‡ nasze Å¼ycie, zaczynaÅ‚o powoli je zabieraÄ‡. KawaÅ‚ek po kawaÅ‚ku.
Wszystko, co piszemy w sieci, przechodzi przez cenzurÄ™. Wszystkie sÅ‚owa, ktÃ³re wypowiadamy, sÄ… podsÅ‚uchiwane, nagrywane, przetwarzane i skÅ‚adowane przez lata. Nie ma juÅ¼ prywatnoÅ›ci i wolnoÅ›ci. W 2024 roku coÅ› poszÅ‚o niezgodnie z planem i musimy to naprawiÄ‡.
Nie wiem, czy moja wizja tego, jak powinien wyglÄ…daÄ‡ Å›wiat, pokrywa siÄ™ z wizjÄ… innych ludzi. NoszÄ™ w sobie jednak obraz Å›wiata idealnego i zrobiÄ™, co mogÄ™, aby ten obraz zrealizowaÄ‡.
Jestem w trakcie rekrutacji kolejnego agenta. Ludzie zarzucajÄ… mi, Å¼e nie powinienem zwracaÄ‡ siÄ™ do nich per 'numer pierwszy' czy 'numer drugi', ale jak inaczej mam mÃ³wiÄ‡ do osÃ³b, ktÃ³re w zasadzie wysyÅ‚am na niemal pewnÄ… Å›mierÄ‡? To jedyny sposÃ³b, aby siÄ™ od nich psychicznie odciÄ…Ä‡ i mÃ³c skupiÄ‡ na wyÅ¼szym celu, Nie mogÄ™ sobie pozwoliÄ‡ na litoÅ›Ä‡ i wspÃ³Å‚czucie.
Niebawem numer piÄ…ty dotrze na szkolenie. PokÅ‚adam w nim caÅ‚Ä… nadziejÄ™, bez jego pomocy misja jest zagroÅ¼ona. Nasze fundusze sÄ… na wyczerpaniu, a moc gÅ‚Ã³wnego generatora pozwoli tylko na jeden skok w czasie. JeÅ›li ponownie Åºle wybraliÅ›my kandydata, oznacza to koniec naszej misji, ale takÅ¼e poczÄ…tek koÅ„ca ludzkoÅ›ci.
dr Zygfryd M.
pl/s"""

# 3. WspÃ³Å‚rzÄ™dne book-cipher
coords = [
    ("A1",53), ("A2",27), ("A2",28), ("A2",29),
    ("A4",5),  ("A4",22), ("A4",23),
    ("A1",13), ("A1",15), ("A1",16), ("A1",17), ("A1",10), ("A1",19),
    ("A2",62), ("A3",31), ("A3",32), ("A1",22), ("A3",34),
    ("A5",37), ("A1",4)
]

lines = text.split("\n")
acts = [re.sub(r"[\.,;:'\"?!]", "", line).split() for line in lines]
raw_flag_fragment = "".join(
    acts[int(a[1]) - 1][s - 1] if 0 <= int(a[1]) - 1 < len(acts) and 0 <= s - 1 < len(acts[int(a[1]) - 1]) else '?' 
    for a, s in coords
)
print(f"[DEBUG] Zaszyfrowany ciÄ…g: {raw_flag_fragment}")

# 4. Przygotowanie promptÃ³w
target_hints = [
    "Å›wiat sprzed setek lat",
    "zdobyty przez ludzi",
    "moÅ¼na przeczytaÄ‡ o tym w ksiÄ…Å¼kach"
]
# Do promptu dorzucamy kontekst legendarnej zatopionej wyspy opisana przez Platona,
# dokÅ‚adnie nakazujemy zwracaÄ‡ jedynie wynik - samÄ… nazwÄ™ krainy.
system_prompt = (
    f"JesteÅ› polskim ekspertem od historii i mitologii. "
    f"Masz zaszyfrowany ciÄ…g '{raw_flag_fragment}'. Ignoruj znaki '?'. "
    f"WskazÃ³wki: {target_hints[0]}, {target_hints[1]}, {target_hints[2]}. "
    "Szukana kraina to legendarna zatopiona wyspa z opowieÅ›ci Platona. "
    "Odpowiadasz zawsze po polsku, uÅ¼ywajÄ…c polskich nazw miejsc i krain. "
    "Podaj tylko polskÄ… nazwÄ™ tej legendarnej krainy."
)
user_prompt = (
    "Podaj polskÄ… nazwÄ™ tej krainy. Odpowiedz jednym sÅ‚owem po polsku."
)

# 5. Konfiguracja klienta LLM z lepszym wykrywaniem modeli
if ENGINE == "openai":
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")
    
    if not OPENAI_API_KEY:
        print("âŒ Brak OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)
        
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)

elif ENGINE == "lmstudio":
    LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
    LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_LM", "llama-3.3-70b-instruct")
    print(f"[DEBUG] LMStudio URL: {LMSTUDIO_API_URL}")
    print(f"[DEBUG] LMStudio Model: {MODEL}")
    from openai import OpenAI
    client = OpenAI(api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL, timeout=60)

elif ENGINE == "anything":
    ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
    ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_ANY", "llama-3.3-70b-instruct")
    print(f"[DEBUG] Anything URL: {ANYTHING_API_URL}")
    print(f"[DEBUG] Anything Model: {MODEL}")
    from openai import OpenAI
    client = OpenAI(api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL, timeout=60)

elif ENGINE == "claude":
    # BezpoÅ›rednia integracja Claude
    try:
        from anthropic import Anthropic
    except ImportError:
        print("âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic", file=sys.stderr)
        sys.exit(1)
    
    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print("âŒ Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr)
        sys.exit(1)
    
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
    print(f"[DEBUG] Claude Model: {MODEL}")
    claude_client = Anthropic(api_key=CLAUDE_API_KEY)

elif ENGINE == "gemini":
    import google.generativeai as genai
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print("âŒ Brak GEMINI_API_KEY w .env", file=sys.stderr)
        sys.exit(1)
    MODEL = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
    print(f"[DEBUG] Gemini Model: {MODEL}")
    genai.configure(api_key=GEMINI_API_KEY)

print(f"âœ… Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL}")

# 6. Funkcja wywoÅ‚ania LLM
def call_llm(sys_p, usr_p):
    if ENGINE in {"openai", "lmstudio", "anything"}:
        print(f"[DEBUG] WysyÅ‚am zapytanie do {ENGINE} z szyfrem")
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[{"role":"system","content":sys_p},{"role":"user","content":usr_p}],
            temperature=0
        )
        # Liczenie tokenÃ³w
        tokens = resp.usage
        print(f"[ğŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]")
        if ENGINE == "openai":
            cost = tokens.prompt_tokens/1_000_000*0.60 + tokens.completion_tokens/1_000_000*2.40
            print(f"[ğŸ’° Koszt OpenAI: {cost:.6f} USD]")
        elif ENGINE in {"lmstudio", "anything"}:
            print(f"[ğŸ’° Model lokalny ({ENGINE}) - brak kosztÃ³w]")
        return resp.choices[0].message.content.strip()
    
    elif ENGINE == "claude":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Claude z szyfrem")
        # Claude - bezpoÅ›rednia integracja
        resp = claude_client.messages.create(
            model=MODEL,
            messages=[{"role":"user","content":sys_p + "\n\n" + usr_p}],
            temperature=0,
            max_tokens=64
        )
        
        # Liczenie tokenÃ³w Claude
        usage = resp.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(f"[ğŸ“Š Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]")
        print(f"[ğŸ’° Koszt Claude: {cost:.6f} USD]")
        
        return resp.content[0].text.strip()
    
    elif ENGINE == "gemini":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Gemini z szyfrem")
        model_llm = genai.GenerativeModel(MODEL)
        resp = model_llm.generate_content([sys_p, usr_p], generation_config={"temperature":0.0,"max_output_tokens":64})
        print(f"[ğŸ“Š Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
        print(f"[ğŸ’° Gemini - sprawdÅº limity w Google AI Studio]")
        return resp.text.strip()

# 7. Main logic
def main():
    print(f"ğŸš€ UÅ¼ywam silnika: {ENGINE}")
    print(f"ğŸ” DekodorujÄ™ szyfr book-cipher...")
    
    # Odczyt odpowiedzi i ekstrakcja flagi â€” obsÅ‚uga <think>â€¦</think>
    raw_name = call_llm(system_prompt, user_prompt)
    print(f"ğŸ¤– OdpowiedÅº modelu: {raw_name}")

    # JeÅ›li jest blok <think>, wyciÄ…gnij tylko to, co po ostatnim </think>
    if "</think>" in raw_name.lower():
        # Pracuj na lowercase, ale zachowaj oryginalny tekst po tagu
        # rsplit na oryginale, nie na lower(), Å¼eby nie traciÄ‡ polskich znakÃ³w itp.
        raw_name = raw_name.rsplit("</think>", 1)[-1].strip()

    # UsuÅ„ wszystkie niepotrzebne biaÅ‚e znaki
    raw_name = raw_name.strip()

    # Szukaj pierwszego sÅ‚owa z polskim alfabetem po tagu (jeÅ›li nie ma, zwrÃ³Ä‡ caÅ‚oÅ›Ä‡)
    match = re.search(r"[A-Za-zÄ„Ä…Ä†Ä‡Ä˜Ä™ÅÅ‚ÅƒÅ„Ã“Ã³ÅšÅ›Å¹ÅºÅ»Å¼]+", raw_name)
    if match:
        name = match.group(0).capitalize()
    else:
        name = raw_name.strip()

    flag = f"FLG{{{name}}}"
    print(f"ğŸ Znaleziona flaga: {flag}")

if __name__ == "__main__":
    main()