#!/usr/bin/env python3
"""
S01E01  (multi-engine + Claude)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Automatyzuje logowanie do systemu robotÃ³w z pytaniem anty-captcha
i rozwiÄ…zuje je za pomocÄ… wybranego silnika LLM
(OpenAI, LM Studio, LocalAI / Anything LLM, Gemini Google, Claude Anthropic).
DODANO: ObsÅ‚ugÄ™ Claude z kompatybilnym interfejsem
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""

from __future__ import annotations

import argparse
import os
import re
import sys
from pathlib import Path
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# POPRAWKA SONARA: Linia 186, 225 - CRITICAL - staÅ‚e zamiast duplikacji literaÅ‚Ã³w
HTML_PARSER = "html.parser"
DIGIT_REGEX = r"(\d{1,4})"

# Import Claude integration (opcjonalny)
try:
    from claude_integration import (add_token_counting_to_openai_call,
                                    setup_claude_for_task)
except ImportError:
    # Kontynuujemy bez Claude - brak komunikatu o bÅ‚Ä™dzie
    pass

# â”€â”€ 0. CLI / env - wybÃ³r silnika â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(override=True)

parser = argparse.ArgumentParser(
    description="Login + captcha solver (multi-engine + Claude)"
)
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

# POPRAWKA: Lepsze wykrywanie silnika
# 1. Argument z CLI
# 2. Zmienna LLM_ENGINE z .env
# 3. Wykrywanie na podstawie MODEL_NAME (ustawianego przez agent.py)
# 4. DomyÅ›lnie lmstudio
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domyÅ›lnie

print(f"ğŸ”„ ENGINE wykryty: {ENGINE}")

# â”€â”€ 1. Dane logowania / staÅ‚e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOGIN_URL = os.getenv("ROBOT_LOGIN_URL")
USERNAME = os.getenv("ROBOT_USERNAME")
PASSWORD = os.getenv("ROBOT_PASSWORD")

if not all([LOGIN_URL, USERNAME, PASSWORD]):
    print(
        "âŒ Brak wymaganych zmiennych: ROBOT_LOGIN_URL, ROBOT_USERNAME, ROBOT_PASSWORD",
        file=sys.stderr,
    )
    sys.exit(1)

ANSI_GREEN = "\033[92m"
ANSI_RESET = "\033[0m"

SYSTEM_PROMPT = "Odpowiedz krÃ³tko: sama liczba / jedno sÅ‚owo, bez wyjaÅ›nieÅ„."

# â”€â”€ 2. Inicjalizacja klienta LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ENGINE == "openai":
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_OPENAI", "gpt-4o-mini"
    )
    if not OPENAI_API_KEY:
        print("âŒ Brak OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)
    from openai import OpenAI

    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)

elif ENGINE == "lmstudio":
    LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
    LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_LM", "llama-3.3-70b-instruct"
    )
    print(f"[DEBUG] LMStudio URL: {LMSTUDIO_API_URL}")
    print(f"[DEBUG] LMStudio Model: {MODEL_NAME}")
    from openai import OpenAI

    client = OpenAI(api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL, timeout=60)

elif ENGINE == "anything":
    ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
    ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_ANY", "llama-3.3-70b-instruct"
    )
    print(f"[DEBUG] Anything URL: {ANYTHING_API_URL}")
    print(f"[DEBUG] Anything Model: {MODEL_NAME}")
    from openai import OpenAI

    client = OpenAI(api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL, timeout=60)

elif ENGINE == "claude":
    # BezpoÅ›rednia integracja Claude
    try:
        from anthropic import Anthropic
    except ImportError:
        print(
            "âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic", file=sys.stderr
        )
        sys.exit(1)

    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print("âŒ Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr)
        sys.exit(1)

    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514"
    )
    print(f"[DEBUG] Claude Model: {MODEL_NAME}")
    claude_client = Anthropic(api_key=CLAUDE_API_KEY)

elif ENGINE == "gemini":
    import google.generativeai as genai

    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print(
            "âŒ Brak GEMINI_API_KEY w .env lub zmiennych Å›rodowiskowych.",
            file=sys.stderr,
        )
        sys.exit(1)
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_GEMINI", "gemini-2.5-pro-latest"
    )
    print(f"[DEBUG] Gemini Model: {MODEL_NAME}")
    genai.configure(api_key=GEMINI_API_KEY)
    model_gemini = genai.GenerativeModel(MODEL_NAME)
else:
    print("âŒ NieobsÅ‚ugiwany silnik:", ENGINE, file=sys.stderr)
    sys.exit(1)

print(f"âœ… Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL_NAME}")


# â”€â”€ 3. Pobranie pytania captcha â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def banner(title: str) -> str:
    if sys.stdout.isatty():
        return f"{ANSI_GREEN}=== {title} ==={ANSI_RESET}"
    return f"=== {title} ==="


def get_question(session: requests.Session, url: str) -> str:
    html = session.get(url).text
    soup = BeautifulSoup(html, HTML_PARSER)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
    div = soup.find(id="human-question")
    if div:
        return div.get_text(strip=True)
    m = re.search(r'<div[^>]*id=["\']human-question["\'][^>]*>([^<]+)</div>', html)
    if m:
        return m.group(1).strip()
    raise ValueError("Nie znaleziono pytania anty-captcha.")


# â”€â”€ 4. Zapytanie LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ask_llm(question: str) -> str:
    if ENGINE in {"openai", "lmstudio", "anything"}:
        print(f"[DEBUG] WysyÅ‚am zapytanie do {ENGINE}: {question}")
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": question},
            ],
            temperature=0,
        )
        raw = resp.choices[0].message.content.strip()
        print(f"[DEBUG] Otrzymana odpowiedÅº: {raw}")

        # Liczenie tokenÃ³w
        tokens = resp.usage
        print(
            f"[ğŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        if ENGINE == "openai":
            cost = (
                tokens.prompt_tokens / 1_000_000 * 0.60
                + tokens.completion_tokens / 1_000_000 * 2.40
            )
            print(f"[ğŸ’° Koszt OpenAI: {cost:.6f} USD]")
        elif ENGINE in {"lmstudio", "anything"}:
            print(f"[ğŸ’° Model lokalny ({ENGINE}) - brak kosztÃ³w]")

        m = re.search(DIGIT_REGEX, raw)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
        return m.group(1) if m else raw

    elif ENGINE == "claude":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Claude: {question}")
        # Claude - bezpoÅ›rednia integracja
        resp = claude_client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": SYSTEM_PROMPT + "\n\n" + question}],
            temperature=0,
            max_tokens=16,
        )
        raw = resp.content[0].text.strip()
        print(f"[DEBUG] Otrzymana odpowiedÅº: {raw}")

        # Liczenie tokenÃ³w Claude
        usage = resp.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(
            f"[ğŸ“Š Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]"
        )
        # POPRAWKA SONARA: Linia 259, 260 - MAJOR - usuniÄ™cie niepotrzebnych f-stringÃ³w
        print("[ğŸ’° Koszt Claude: {:.6f} USD]".format(cost))

        m = re.search(DIGIT_REGEX, raw)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
        return m.group(1) if m else raw

    elif ENGINE == "gemini":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Gemini: {question}")
        response = model_gemini.generate_content(
            [SYSTEM_PROMPT, question],
            generation_config={"temperature": 0.0, "max_output_tokens": 16},
        )
        raw = response.text.strip()
        print(f"[DEBUG] Otrzymana odpowiedÅº: {raw}")
        print("[ğŸ“Š Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
        print("[ğŸ’° Gemini - sprawdÅº limity w Google AI Studio]")
        m = re.search(DIGIT_REGEX, raw)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
        return m.group(1) if m else raw


# â”€â”€ 5. Logowanie i pobranie strony sekretu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def login_and_get_secret(sess: requests.Session, answer: str):
    data = {"username": USERNAME, "password": PASSWORD, "answer": answer}
    headers = {"Content-Type": "application/x-www-form-urlencoded"}
    r = sess.post(LOGIN_URL, data=data, headers=headers, allow_redirects=True)
    r.raise_for_status()
    url = r.url
    if url.rstrip("/") == LOGIN_URL.rstrip("/"):
        soup = BeautifulSoup(r.text, HTML_PARSER)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
        a = soup.find("a", href=re.compile(r"secret", re.I))
        if not a:
            raise ValueError("Nie znaleziono linku do sekretnej strony.")
        url = urljoin(LOGIN_URL, a["href"])
        r = sess.get(url)
        r.raise_for_status()
    return url, r.text


# â”€â”€ 6. Ekstrakcja flagi i plikÃ³w .txt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_flag(html: str) -> str:
    for pat in (r"\{\{FLG:[^}]+\}\}", r"FLG\{[^}]+\}"):
        m = re.search(pat, html)
        if m:
            return m.group(0)
    raise ValueError("Nie znaleziono flagi.")


def fetch_txt_files(sess: requests.Session, secret_html: str):
    soup = BeautifulSoup(secret_html, HTML_PARSER)  # POPRAWKA SONARA: uÅ¼ycie staÅ‚ej
    hrefs = {
        a["href"]
        for a in soup.find_all("a", href=True)
        if a["href"].startswith("/files/") and a["href"].lower().endswith(".txt")
    }
    for dt in soup.find_all("dt"):
        m = re.search(r"Version (\d+\.\d+\.\d+)", dt.get_text(strip=True))
        if m:
            ver = m.group(1).replace(".", "_")
            hrefs.add(f"/files/{ver}.txt")
    for href in sorted(hrefs):
        url = urljoin(LOGIN_URL, href)
        try:
            resp = sess.get(url)
            resp.raise_for_status()
            yield url, resp.text
        except requests.HTTPError as e:
            if e.response.status_code == 404:
                print(f"[!] Brak pliku {url} (404)", file=sys.stderr)
            else:
                raise


# â”€â”€ 7. Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    print(f"ğŸ”„ Engine: {ENGINE}")
    with requests.Session() as s:
        print(banner("Pytanie captcha"))
        q = get_question(s, LOGIN_URL)
        print(q)
        print(banner("OdpowiedÅº LLM"))
        a = ask_llm(q)
        print(a)
        print(banner("Logowanie"))
        url, html = login_and_get_secret(s, a)
        print(f"URL sekretu: {url}")
        print(banner("PeÅ‚ny HTML"))
        print(html)
        print(banner("Koniec HTML"))
        print(banner("Flaga"))
        print(extract_flag(html))
        print(banner("Pliki .txt"))
        for file_url, content in fetch_txt_files(s, html):
            print(f"\n-- {file_url} --\n{content}\n")


# â”€â”€ 8. Uruchom â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    try:
        main()
    except Exception as err:
        print(f"[BÅÄ„D] {err}", file=sys.stderr)
        sys.exit(1)