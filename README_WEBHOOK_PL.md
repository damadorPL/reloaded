# Drone Navigation Webhook

RozwiƒÖzanie zadania "webhook" (Zadanie 18) - API interpretujƒÖce instrukcje lotu drona po mapie 4x4 z orkiestracjƒÖ LangGraph.

## Opis zadania

1. **Mapa 4x4** - dron porusza siƒô po siatce:
   ```
   [1,1] Start    [1,2] ≈ÅƒÖka     [1,3] Drzewo    [1,4] Dom
   [2,1] ≈ÅƒÖka     [2,2] Wiatrak  [2,3] ≈ÅƒÖka      [2,4] ≈ÅƒÖka
   [3,1] ≈ÅƒÖka     [3,2] ≈ÅƒÖka     [3,3] Ska≈Çy     [3,4] Drzewa
   [4,1] G√≥ry     [4,2] G√≥ry     [4,3] Samoch√≥d  [4,4] Jaskinia
   ```

2. **API** przyjmuje instrukcje w jƒôzyku naturalnym i zwraca co znajduje siƒô na ko≈Ñcowej pozycji
3. **Dron zawsze startuje** z pozycji [1,1] (lewy g√≥rny r√≥g)

## Szybki start

### 1. Przygotowanie ≈õrodowiska

```bash
# Edytuj .env i ustaw klucze API
nano .env

# Instaluj zale≈ºno≈õci
pip install fastapi uvicorn python-dotenv langgraph langchain-core requests pydantic openai

# Dla innych silnik√≥w:
pip install anthropic  # dla Claude
pip install google-generativeai  # dla Gemini
pip install langchain-anthropic  # dla Claude z agent.py
```

### 2. Instalacja ngrok

```bash
# macOS
brew install ngrok

# Ubuntu
snap install ngrok

# Lub pobierz z https://ngrok.com/download

# WSL / Linux bez snap
curl -sSL https://ngrok-agent.s3.amazonaws.com/ngrok.asc \
  | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null \
  && echo "deb https://ngrok-agent.s3.amazonaws.com buster main" \
  | sudo tee /etc/apt/sources.list.d/ngrok.list \
  && sudo apt update \
  && sudo apt install ngrok

# TOKEN - za≈Ç√≥≈º konto i wygeneruj token: https://dashboard.ngrok.com/signup
ngrok config add-authtoken YOUR_TOKEN
```

### 3. Uruchomienie webhook

#### Opcja A: Bezpo≈õrednie uruchomienie z obs≈ÇugƒÖ wielu silnik√≥w (zalecane)

```bash
# OpenAI (domy≈õlne)
python zad18.py --engine openai

# Claude 
python zad18.py --engine claude

# Gemini
python zad18.py --engine gemini

# Lokalne modele (LMStudio)
python zad18.py --engine lmstudio

# Anything LLM
python zad18.py --engine anything
```

#### Opcja B: Przez agenta (dla czystego wyciƒÖgniƒôcia flagi)

```bash
# Uruchom agenta
python agent.py

# Wybierz silnik gdy zostaniesz poproszony, nastƒôpnie uruchom:
> run_task 18

# Agent zwr√≥ci tylko flagƒô: {{FLG:}}
```

#### Opcja C: Niestandardowe opcje

```bash
# U≈ºyj innego portu
python zad18.py --engine claude --port 3002

# Nie wysy≈Çaj URL automatycznie (rƒôczne testowanie)
python zad18.py --engine openai --skip-send
```

### 4. Testowanie lokalne

```bash
# Testuj pojedynczƒÖ instrukcjƒô gdy webhook dzia≈Ça
curl -X POST http://localhost:3001/ \
  -H "Content-Type: application/json" \
  -d '{"instruction":"id≈∫ w prawo i w d√≥≈Ç"}'

# Oczekiwana odpowied≈∫:
# {"description":"wiatrak","_thinking":"..."}
```

## Architektura wielu silnik√≥w

Webhook automatycznie wykrywa i u≈ºywa okre≈õlonego silnika LLM:

```python
# Obs≈Çugiwane silniki:
--engine openai     # Modele GPT przez OpenAI API
--engine claude     # Modele Claude przez Anthropic API  
--engine gemini     # Modele Gemini przez Google API
--engine lmstudio   # Lokalne modele przez LMStudio
--engine anything   # Lokalne modele przez Anything LLM
```

### Pipeline LangGraph

```python
[START]
   ‚Üì
[check_environment] - Sprawd≈∫ ngrok i dostƒôpno≈õƒá portu
   ‚Üì
[start_server] - Uruchom webhook FastAPI w tle
   ‚Üì
[start_ngrok] - Stw√≥rz publiczny tunel i pobierz URL
   ‚Üì
[send_webhook_url] - Wy≈õlij URL do serwera centralnego
   ‚Üì
[wait_for_completion] - Czekaj na testy (lub auto-wyj≈õcie po fladze)
   ‚Üì
[cleanup] - Zatrzymaj wszystkie procesy
   ‚Üì
[END]
```

### Nawigacja drona z LangGraph

```python
[START] 
   ‚Üì
[parse_instruction] - LLM interpretuje instrukcjƒô na ruchy
   ‚Üì
[execute_movements] - Symuluje ruch drona po mapie
   ‚Üì
[END] ‚Üí zwraca opis miejsca
```

### Kluczowe komponenty:

1. **WebhookState** - stan pipeline webhook w LangGraph
2. **NavigationState** - stan przep≈Çywu nawigacji drona
3. **parse\_instruction\_node** - u≈ºywa LLM do interpretacji instrukcji
4. **execute\_movements\_node** - wykonuje ruchy i sprawdza pozycjƒô
5. **FastAPI endpoints** - `/` dla instrukcji, `/health` dla sprawdzenia

## üîß Konfiguracja

### Zmienne ≈õrodowiskowe (.env)

```bash
# Serwer centralny
CENTRALA_API_KEY=tw√≥j-klucz
REPORT_URL=https://xxx.xxx.xxx/report

# Silnik LLM (opcjonalne - mo≈ºna ustawiƒá przez --engine)
LLM_ENGINE=openai  # lub: claude, lmstudio, gemini, anything

# Klucze API (w zale≈ºno≈õci od silnika)
OPENAI_API_KEY=sk-...
CLAUDE_API_KEY=sk-ant-...
ANTHROPIC_API_KEY=sk-ant-...  # alternatywa do CLAUDE_API_KEY
GEMINI_API_KEY=AI...

# Modele dla ka≈ºdego silnika
MODEL_NAME_OPENAI=gpt-4o-mini
MODEL_NAME_CLAUDE=claude-sonnet-4-20250514
MODEL_NAME_GEMINI=gemini-2.5-pro-latest
MODEL_NAME_LM=llama-3.3-70b-instruct
MODEL_NAME_ANY=llama-3.3-70b-instruct

# URL lokalnych modeli
LMSTUDIO_API_URL=http://localhost:1234/v1
LMSTUDIO_API_KEY=local
ANYTHING_API_URL=http://localhost:1234/v1
ANYTHING_API_KEY=local
```

## Opcje linii polece≈Ñ

```bash
python zad18.py [OPCJE]

Opcje:
  --engine {openai,claude,gemini,lmstudio,anything}
                        Backend LLM do u≈ºycia
  --port INTEGER        Port dla serwera webhook (domy≈õlnie: 3001)
  --skip-send          Nie wysy≈Çaj URL do serwera centralnego automatycznie
  --help               Poka≈º tƒô wiadomo≈õƒá i wyjd≈∫
```

## Przyk≈Çady instrukcji

Webhook obs≈Çuguje z≈Ço≈ºone instrukcje w jƒôzyku naturalnym:

1. **Proste**: "id≈∫ w prawo", "poleƒá w d√≥≈Ç"
2. **Z≈Ço≈ºone**: "na maksa w prawo, a p√≥≈∫niej ile wlezie w d√≥≈Ç"
3. **Z anulowaniem**: "leƒá w d√≥≈Ç, albo nie! czekaj! w prawo"
4. **Wieloetapowe**: "w prawo, potem w d√≥≈Ç, potem jeszcze raz w prawo"
5. **Angielskie przyk≈Çady**: "all the way right, then as far down as possible"

## RozwiƒÖzywanie problem√≥w

### "Port ju≈º zajƒôty"
```bash
# Znajd≈∫ proces
lsof -i :3001

# Zabij proces
kill -9 <PID>
```

### "Ngrok nie dzia≈Ça"
* Sprawd≈∫ czy masz konto ngrok (darmowe)
* Ustaw authtoken: `ngrok config add-authtoken <tw√≥j-token>`
* Sprawd≈∫ czy ngrok dzia≈Ça: `curl http://localhost:4040/api/tunnels`

### "LLM nie interpretuje poprawnie"
* Sprawd≈∫ model w .env (GPT-4 dzia≈Ça lepiej ni≈º GPT-3.5)
* Zweryfikuj czy klucze API sƒÖ ustawione poprawnie
* Dla lokalnych modeli, upewnij siƒô ≈ºe LMStudio/Anything dzia≈Ça

### "Silnik nie znaleziony"
* Zainstaluj wymagane pakiety:
  ```bash
  pip install anthropic  # dla Claude
  pip install google-generativeai  # dla Gemini
  pip install langchain-anthropic  # dla agent.py z Claude
  ```

## Struktura odpowiedzi

API zawsze zwraca:
```json
{
    "description": "nazwa_miejsca",
    "_thinking": "opcjonalne_logi_debugowania"
}
```

Gdzie `description` to maksymalnie 2 s≈Çowa opisujƒÖce miejsce docelowe.

## Uzyskanie flagi

### Metoda 1: Bezpo≈õrednie uruchomienie
```bash
python zad18.py --engine claude
# Webhook automatycznie zako≈Ñczy siƒô po otrzymaniu: {{FLG:}}
```

### Metoda 2: Przez agenta (czyste wyj≈õcie)
```bash
python agent.py
> run_task 18
# Zwraca tylko: üèÅ Flaga znaleziona: {{FLG:}} - ko≈Ñczƒô zadanie.
```

## Przep≈Çyw procesu

1. **Sprawdzenie ≈õrodowiska** - Weryfikacja instalacji ngrok i dostƒôpno≈õci portu
2. **Uruchomienie serwera** - Start serwera webhook FastAPI w wƒÖtku t≈Ça
3. **Tworzenie tunelu** - Uruchomienie ngrok i pobranie publicznego URL HTTPS
4. **Rejestracja** - Wys≈Çanie URL webhook do serwera centralnego
5. **Faza testowania** - Serwer centralny testuje webhook r√≥≈ºnymi instrukcjami
6. **Otrzymanie flagi** - Je≈õli wszystkie testy przejdƒÖ, otrzymaj flagƒô i auto-wyj≈õcie
7. **SprzƒÖtanie** - Zatrzymanie wszystkich proces√≥w i czyste wyj≈õcie

Ca≈Çy proces jest zautomatyzowany i zako≈Ñczy siƒô bez ingerencji u≈ºytkownika po uruchomieniu.