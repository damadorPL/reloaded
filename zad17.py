#!/usr/bin/env python3
"""
S04E03 - Uniwersalny mechanizm przeszukiwania stron internetowych
Zadanie: Odpowiedz na pytania centrali przeszukujÄ…c stronÄ™ firmy SoftoAI
"""
import argparse
import os
import sys
import requests
import json
import re
import time
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import TypedDict, Optional, List, Dict, Any, Set
from langgraph.graph import StateGraph, START, END
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import html2text

# Konfiguracja loggera
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# 1. Konfiguracja i wykrywanie silnika
load_dotenv(override=True)

parser = argparse.ArgumentParser(description="Uniwersalny agent przeszukujÄ…cy strony (multi-engine)")
parser.add_argument("--engine", choices=["openai", "lmstudio", "anything", "gemini", "claude"],
                    help="LLM backend to use")
args = parser.parse_args()

ENGINE: Optional[str] = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"âŒ NieobsÅ‚ugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"ğŸ”„ ENGINE wykryty: {ENGINE}")

# Sprawdzenie zmiennych Å›rodowiskowych
REPORT_URL: str = os.getenv("REPORT_URL")
CENTRALA_API_KEY: str = os.getenv("CENTRALA_API_KEY")
QUESTIONS_URL: str = os.getenv("SOFTO_QUESTIONS_URL")
SOFTO_URL: str = os.getenv("SOFTO_URL")

if not all([REPORT_URL, CENTRALA_API_KEY, QUESTIONS_URL, SOFTO_URL]):
    print("âŒ Brak wymaganych zmiennych: REPORT_URL, CENTRALA_API_KEY, SOFTO_QUESTIONS_URL, SOFTO_URL", file=sys.stderr)
    sys.exit(1)


# Konfiguracja modelu
if ENGINE == "openai":
    MODEL_NAME: str = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")
elif ENGINE == "claude":
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
elif ENGINE == "gemini":
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
elif ENGINE == "lmstudio":
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_LM", "llama-3.3-70b-instruct")
elif ENGINE == "anything":
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_ANY", "llama-3.3-70b-instruct")

print(f"âœ… Model: {MODEL_NAME}")

# 2. Inicjalizacja klienta LLM
def call_llm(prompt: str, temperature: float = 0) -> str:
    """Uniwersalna funkcja wywoÅ‚ania LLM"""
    
    if ENGINE == "openai":
        from openai import OpenAI
        client = OpenAI(
            api_key=os.getenv('OPENAI_API_KEY'),
            base_url=os.getenv('OPENAI_API_URL') or None
        )
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    
    elif ENGINE == "claude":
        try:
            from anthropic import Anthropic
        except ImportError:
            print("âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic", file=sys.stderr)
            sys.exit(1)
        
        client = Anthropic(api_key=os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"))
        resp = client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=1000
        )
        return resp.content[0].text.strip()
    
    elif ENGINE in {"lmstudio", "anything"}:
        from openai import OpenAI
        base_url = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1") if ENGINE == "lmstudio" else os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
        api_key = os.getenv("LMSTUDIO_API_KEY", "local") if ENGINE == "lmstudio" else os.getenv("ANYTHING_API_KEY", "local")
        
        client = OpenAI(api_key=api_key, base_url=base_url)
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature
        )
        return resp.choices[0].message.content.strip()
    
    elif ENGINE == "gemini":
        import google.generativeai as genai
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        model = genai.GenerativeModel(MODEL_NAME)
        response = model.generate_content(
            [prompt],
            generation_config={"temperature": temperature, "max_output_tokens": 1000}
        )
        return response.text.strip()

# 3. Typowanie stanu pipeline
class SearchState(TypedDict):
    """Stan wyszukiwania dla pojedynczego pytania"""
    question_id: str
    question_text: str
    current_url: str
    visited_urls: Set[str]
    search_depth: int
    answer: Optional[str]
    found: bool

class PipelineState(TypedDict, total=False):
    questions: Dict[str, str]
    search_states: Dict[str, SearchState]
    answers: Dict[str, str]
    result: Optional[str]

# 4. Funkcje pomocnicze
def fetch_questions() -> Optional[Dict[str, str]]:
    """Pobiera pytania z API centrali"""
    logger.info(f"ğŸ“¥ Pobieranie pytaÅ„ z centrali...")
    
    try:
        response = requests.get(QUESTIONS_URL)
        response.raise_for_status()
        questions = response.json()
        logger.info(f"âœ… Pobrano {len(questions)} pytaÅ„")
        return questions
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania pytaÅ„: {e}")
        return None

def fetch_webpage(url: str) -> Optional[str]:
    """Pobiera zawartoÅ›Ä‡ strony internetowej"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d pobierania strony {url}: {e}")
        return None

def html_to_markdown(html_content: str) -> str:
    """Konwertuje HTML na Markdown"""
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = True
    h.body_width = 0  # Nie zawijaj tekstu
    return h.handle(html_content)

def extract_links(html_content: str, base_url: str) -> List[Dict[str, str]]:
    """Ekstraktuje linki ze strony HTML"""
    soup = BeautifulSoup(html_content, 'html.parser')
    links = []
    
    for a in soup.find_all('a', href=True):
        href = a['href']
        text = a.get_text(strip=True)
        
        # PomiÅ„ kotwice i JavaScript
        if href.startswith('#') or href.startswith('javascript:'):
            continue
        
        # UtwÃ³rz peÅ‚ny URL
        full_url = urljoin(base_url, href)
        
        # SprawdÅº czy link jest w domenie softo
        parsed = urlparse(full_url)
        if parsed.netloc and 'softo.ag3nts.org' not in parsed.netloc:
            continue
        
        links.append({
            'url': full_url,
            'text': text or href
        })
    
    return links

def check_for_answer(content: str, question: str) -> Optional[str]:
    """Sprawdza czy strona zawiera odpowiedÅº na pytanie"""
    prompt = f"""Przeanalizuj poniÅ¼szÄ… treÅ›Ä‡ strony i odpowiedz czy zawiera ona odpowiedÅº na pytanie.

Pytanie: {question}

TreÅ›Ä‡ strony:
{content[:4000]}  # Ograniczenie dÅ‚ugoÅ›ci

Instrukcje:
1. JeÅ›li strona zawiera konkretnÄ… odpowiedÅº na pytanie, zwrÃ³Ä‡ TYLKO tÄ™ odpowiedÅº (bez dodatkowych wyjaÅ›nieÅ„).
2. JeÅ›li strona nie zawiera odpowiedzi, zwrÃ³Ä‡ dokÅ‚adnie: "BRAK ODPOWIEDZI"

OdpowiedÅº:"""
    
    response = call_llm(prompt)
    
    if "BRAK ODPOWIEDZI" in response.upper():
        return None
    
    # OczyÅ›Ä‡ odpowiedÅº z niepotrzebnych elementÃ³w
    answer = response.strip()
    
    # UsuÅ„ typowe prefiksy
    prefixes_to_remove = [
        "OdpowiedÅº na pytanie to:",
        "OdpowiedÅº:",
        "OdpowiedÅº to:",
        "Adres e-mail to:",
        "Email to:",
        "Adres email:",
    ]
    
    for prefix in prefixes_to_remove:
        if answer.lower().startswith(prefix.lower()):
            answer = answer[len(prefix):].strip()
    
    return answer

def select_best_link(links: List[Dict[str, str]], question: str, visited_urls: Set[str]) -> Optional[str]:
    """Wybiera najlepszy link do dalszego przeszukiwania"""
    # Filtruj odwiedzone linki
    unvisited_links = [link for link in links if link['url'] not in visited_urls]
    
    if not unvisited_links:
        return None
    
    # Przygotuj listÄ™ linkÃ³w dla LLM
    links_text = "\n".join([f"{i+1}. {link['text']} -> {link['url']}" for i, link in enumerate(unvisited_links)])
    
    prompt = f"""AnalizujÄ™ stronÄ™ w poszukiwaniu odpowiedzi na pytanie:
{question}

DostÄ™pne linki:
{links_text}

KtÃ³ry link najprawdopodobniej doprowadzi do odpowiedzi? RozwaÅ¼:
1. TreÅ›Ä‡ linku i jego zwiÄ…zek z pytaniem
2. TypowÄ… strukturÄ™ stron (np. "Kontakt" dla adresÃ³w email, "O nas" dla informacji o firmie)
3. Unikaj linkÃ³w ktÃ³re wyglÄ…dajÄ… na puÅ‚apki (np. loop, endless, czescizamienne)

ZwrÃ³Ä‡ TYLKO numer linku (np. "3") bez Å¼adnych dodatkowych wyjaÅ›nieÅ„."""
    
    response = call_llm(prompt)
    
    # WyciÄ…gnij numer
    try:
        number = int(re.search(r'\d+', response).group())
        if 1 <= number <= len(unvisited_links):
            return unvisited_links[number - 1]['url']
    except:
        pass
    
    # JeÅ›li nie udaÅ‚o siÄ™, zwrÃ³Ä‡ pierwszy nieodwiedzony
    return unvisited_links[0]['url'] if unvisited_links else None

# 5. Nodes dla LangGraph
def fetch_questions_node(state: PipelineState) -> PipelineState:
    """Pobiera pytania z centrali"""
    questions = fetch_questions()
    if not questions:
        logger.error("âŒ Nie udaÅ‚o siÄ™ pobraÄ‡ pytaÅ„")
        return state
    
    state["questions"] = questions
    
    # Inicjalizuj stany wyszukiwania dla kaÅ¼dego pytania
    search_states = {}
    for q_id, q_text in questions.items():
        search_states[q_id] = SearchState(
            question_id=q_id,
            question_text=q_text,
            current_url=SOFTO_URL,
            visited_urls=set(),
            search_depth=0,
            answer=None,
            found=False
        )
    
    state["search_states"] = search_states
    state["answers"] = {}
    
    return state

def search_answers_node(state: PipelineState) -> PipelineState:
    """Przeszukuje strony w poszukiwaniu odpowiedzi"""
    search_states = state.get("search_states", {})
    answers = state.get("answers", {})
    
    max_depth = 10  # Maksymalna gÅ‚Ä™bokoÅ›Ä‡ przeszukiwania
    
    for q_id, search_state in search_states.items():
        if search_state["found"]:
            continue
        
        logger.info(f"\nğŸ” Szukam odpowiedzi na pytanie {q_id}: {search_state['question_text']}")
        
        while search_state["search_depth"] < max_depth and not search_state["found"]:
            current_url = search_state["current_url"]
            
            # SprawdÅº czy juÅ¼ odwiedziliÅ›my tÄ™ stronÄ™
            if current_url in search_state["visited_urls"]:
                logger.warning(f"âš ï¸  Strona juÅ¼ odwiedzona: {current_url}")
                break
            
            logger.info(f"ğŸ“„ Pobieram stronÄ™: {current_url}")
            
            # Pobierz stronÄ™
            html_content = fetch_webpage(current_url)
            if not html_content:
                break
            
            # Oznacz jako odwiedzonÄ…
            search_state["visited_urls"].add(current_url)
            
            # Konwertuj na Markdown
            markdown_content = html_to_markdown(html_content)
            
            # SprawdÅº czy jest odpowiedÅº
            answer = check_for_answer(markdown_content, search_state["question_text"])
            
            if answer:
                logger.info(f"âœ… Znaleziono odpowiedÅº: {answer}")
                search_state["answer"] = answer
                search_state["found"] = True
                answers[q_id] = answer
                break
            
            # JeÅ›li nie ma odpowiedzi, wybierz nastÄ™pny link
            logger.info("âŒ Brak odpowiedzi na tej stronie")
            
            # WyciÄ…gnij linki
            links = extract_links(html_content, current_url)
            logger.info(f"ğŸ”— Znaleziono {len(links)} linkÃ³w")
            
            # Wybierz najlepszy link
            next_url = select_best_link(links, search_state["question_text"], search_state["visited_urls"])
            
            if not next_url:
                logger.warning("âš ï¸  Brak wiÄ™cej linkÃ³w do sprawdzenia")
                break
            
            logger.info(f"â¡ï¸  PrzechodzÄ™ do: {next_url}")
            search_state["current_url"] = next_url
            search_state["search_depth"] += 1
            
            # KrÃ³tka przerwa aby nie przeciÄ…Å¼aÄ‡ serwera
            time.sleep(0.5)
        
        if not search_state["found"]:
            logger.warning(f"âš ï¸  Nie znaleziono odpowiedzi na pytanie {q_id}")
    
    state["answers"] = answers
    return state

def send_answers_node(state: PipelineState) -> PipelineState:
    """WysyÅ‚a odpowiedzi do centrali"""
    answers = state.get("answers", {})
    
    if not answers:
        logger.error("âŒ Brak odpowiedzi do wysÅ‚ania")
        return state
    
    # Przygotuj payload
    payload = {
        "task": "softo",
        "apikey": CENTRALA_API_KEY,
        "answer": answers
    }
    
    logger.info(f"ğŸ“¤ WysyÅ‚am odpowiedzi do centrali...")
    logger.info(f"Odpowiedzi: {json.dumps(answers, indent=2, ensure_ascii=False)}")
    
    try:
        response = requests.post(REPORT_URL, json=payload)
        response.raise_for_status()
        result = response.json()
        logger.info(f"âœ… OdpowiedÅº centrali: {result}")
        
        # SprawdÅº czy jest flaga
        if "FLG" in str(result):
            print(f"ğŸ {result.get('message', result)}")
            state["result"] = result.get("message", str(result))
        
    except Exception as e:
        logger.error(f"âŒ BÅ‚Ä…d wysyÅ‚ania: {e}")
        if hasattr(e, 'response') and e.response:
            logger.error(f"SzczegÃ³Å‚y: {e.response.text}")
    
    return state

def build_graph() -> Any:
    """Buduje graf LangGraph"""
    graph = StateGraph(state_schema=PipelineState)
    
    # Dodaj nodes
    graph.add_node("fetch_questions", fetch_questions_node)
    graph.add_node("search_answers", search_answers_node)
    graph.add_node("send_answers", send_answers_node)
    
    # Dodaj edges
    graph.add_edge(START, "fetch_questions")
    graph.add_edge("fetch_questions", "search_answers")
    graph.add_edge("search_answers", "send_answers")
    graph.add_edge("send_answers", END)
    
    return graph.compile()

def main() -> None:
    print("=== Uniwersalny Agent PrzeszukujÄ…cy Strony ===")
    print(f"ğŸš€ UÅ¼ywam silnika: {ENGINE}")
    print(f"ğŸ”§ Model: {MODEL_NAME}")
    print(f"ğŸŒ Strona do przeszukania: {SOFTO_URL}")
    print("Startuje pipeline...\n")
    
    try:
        graph = build_graph()
        result: PipelineState = graph.invoke({})
        
        if result.get("result"):
            print(f"\nğŸ‰ Zadanie zakoÅ„czone!")
        else:
            print("\nâœ… Proces zakoÅ„czony")
            
            # PokaÅ¼ podsumowanie
            answers = result.get("answers", {})
            if answers:
                print(f"\nğŸ“Š Znaleziono {len(answers)} odpowiedzi:")
                for q_id, answer in answers.items():
                    print(f"   {q_id}: {answer}")
            else:
                print("\nâŒ Nie znaleziono Å¼adnych odpowiedzi")
                
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
