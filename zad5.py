#!/usr/bin/env python3
"""
S02E01 - Analiza nagra≈Ñ audio z przes≈Çucha≈Ñ
Obs≈Çuguje: openai, lmstudio, anything, gemini, claude
DODANO: Obs≈Çugƒô Claude z bezpo≈õredniƒÖ integracjƒÖ (jak zad1.py i zad2.py)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""
import argparse
import os
import sys
import zipfile
from pathlib import Path

import requests
from dotenv import load_dotenv

# Konfiguracja i helpery
load_dotenv(override=True)

# POPRAWKA: Dodano argumenty CLI jak w innych zadaniach
parser = argparse.ArgumentParser(
    description="Analiza audio z przes≈Çucha≈Ñ (multi-engine + Claude)"
)
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude"],
    help="LLM backend to use",
)
args = parser.parse_args()

# POPRAWKA: Lepsze wykrywanie silnika (jak w poprawionych zad1.py-zad4.py)
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # Pr√≥buj wykryƒá silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # Sprawd≈∫ kt√≥re API keys sƒÖ dostƒôpne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domy≈õlnie

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"‚ùå Nieobs≈Çugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"üîÑ ENGINE wykryty: {ENGINE}")

DATA_URL = os.getenv("DATA_URL")
REPORT_URL = os.getenv("REPORT_URL")
CENTRALA_KEY = os.getenv("CENTRALA_API_KEY")

if not all([DATA_URL, REPORT_URL, CENTRALA_KEY]):
    print(
        "‚ùå Brak wymaganych zmiennych: DATA_URL, REPORT_URL, CENTRALA_API_KEY",
        file=sys.stderr,
    )
    sys.exit(1)

# POPRAWKA: Inicjalizacja LLM z lepszym wykrywaniem modeli
if ENGINE == "openai":
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_OPENAI", "gpt-4o-mini"
    )

    if not OPENAI_API_KEY:
        print("‚ùå Brak OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)

    from openai import OpenAI

    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)

elif ENGINE == "lmstudio":
    LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
    LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_LM", "llama-3.3-70b-instruct"
    )
    print(f"[DEBUG] LMStudio URL: {LMSTUDIO_API_URL}")
    print(f"[DEBUG] LMStudio Model: {MODEL_NAME}")
    from openai import OpenAI

    client = OpenAI(
        api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL, timeout=120
    )  # Zwiƒôkszony timeout

elif ENGINE == "anything":
    ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
    ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_ANY", "llama-3.3-70b-instruct"
    )
    print(f"[DEBUG] Anything URL: {ANYTHING_API_URL}")
    print(f"[DEBUG] Anything Model: {MODEL_NAME}")
    from openai import OpenAI

    client = OpenAI(
        api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL, timeout=120
    )  # Zwiƒôkszony timeout

elif ENGINE == "claude":
    # Bezpo≈õrednia integracja Claude
    try:
        from anthropic import Anthropic
    except ImportError:
        print(
            "‚ùå Musisz zainstalowaƒá anthropic: pip install anthropic", file=sys.stderr
        )
        sys.exit(1)

    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print("‚ùå Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr)
        sys.exit(1)

    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514"
    )
    print(f"[DEBUG] Claude Model: {MODEL_NAME}")
    claude_client = Anthropic(api_key=CLAUDE_API_KEY)

elif ENGINE == "gemini":
    import google.generativeai as genai

    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print("‚ùå Brak GEMINI_API_KEY w .env", file=sys.stderr)
        sys.exit(1)
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv(
        "MODEL_NAME_GEMINI", "gemini-2.5-pro-latest"
    )
    print(f"[DEBUG] Gemini Model: {MODEL_NAME}")
    genai.configure(api_key=GEMINI_API_KEY)
    model_gemini = genai.GenerativeModel(MODEL_NAME)

print(f"‚úÖ Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL_NAME}")


def download_and_extract_zip(url: str, dest: Path) -> None:
    dest.mkdir(parents=True, exist_ok=True)
    zip_path = dest / "przesluchania.zip"
    resp = requests.get(url, stream=True)
    resp.raise_for_status()
    with open(zip_path, "wb") as f:
        for chunk in resp.iter_content(8192):
            f.write(chunk)
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(dest)
    zip_path.unlink()


def find_audio_files(root: Path, exts=None) -> list[Path]:
    if exts is None:
        exts = [".m4a", ".mp3", ".wav", ".flac"]
    return sorted(
        p for p in root.rglob("*") if p.is_file() and p.suffix.lower() in exts
    )


def get_transcript(audio_path: Path) -> str:
    """Pobiera transkrypcjƒô z cache lub generuje nowƒÖ via Whisper i zapisuje lokalnie."""
    txt_path = audio_path.with_suffix(".txt")
    if txt_path.exists():
        print(f"   > U≈ºywam zapisanej transkrypcji: {txt_path.name}")
        return txt_path.read_text(encoding="utf-8")

    # brak cache, generujemy transkrypcjƒô (tylko OpenAI Whisper)
    if ENGINE in {"gemini", "claude"}:
        print(f"‚ùå Transkrypcja audio (Whisper) nie jest dostƒôpna dla {ENGINE}.")
        print("üí° U≈ºyj --engine openai, lmstudio lub anything dla transkrypcji audio.")
        sys.exit(1)

    print(f"   > Transkrypcja z API dla: {audio_path.name}")
    with open(audio_path, "rb") as f:
        if ENGINE in {"lmstudio", "anything"}:
            # Lokalne modele mogƒÖ mieƒá inny endpoint dla audio
            transcribe_url = os.getenv("TRANSCRIBE_API_URL", "http://localhost:1234/v1")
            transcribe_client = OpenAI(
                api_key=os.getenv(
                    "LMSTUDIO_API_KEY" if ENGINE == "lmstudio" else "ANYTHING_API_KEY",
                    "local",
                ),
                base_url=transcribe_url,
            )
            resp = transcribe_client.audio.transcriptions.create(
                file=f,
                model="whisper-1",  # lub lokalny model
                response_format="text",
                language="pl",
            )
        else:  # openai
            resp = client.audio.transcriptions.create(
                file=f, model="whisper-1", response_format="text", language="pl"
            )

    text = getattr(resp, "text", resp)
    txt_path.write_text(text, encoding="utf-8")
    return text


def infer_answer(fragments: str) -> str:
    system_msg = (
        "Jeste≈õ ≈õledczym-logiki."
        "Otrzymasz dwa fragmenty zezna≈Ñ dotyczƒÖce przedmiotu i miejsca wyk≈Çad√≥w."
        '1. Wypisz "Fakt 1" - przedmiot wyk≈Çad√≥w.'
        '2. Wypisz "Fakt 2" - miasto wyk≈Çad√≥w.'
        "3. Na podstawie tych fakt√≥w wnioskuj nazwƒô wydzia≈Çu uczelni."
        "4. KorzystajƒÖc z wiedzy og√≥lnej, podaj ulicƒô siedziby wydzia≈Çu."
        "Najpierw rozpisz chain-of-thought, potem odpowied≈∫ w formacie:"
        "Wydzia≈Ç: <pe≈Çna nazwa>Ulica: <ulica i numer>"
    )
    user_msg = f"Fragmenty zezna≈Ñ:{fragments}"

    if ENGINE in {"openai", "lmstudio", "anything"}:
        print(f"[DEBUG] Wysy≈Çam zapytanie do {ENGINE} z fragmentami zezna≈Ñ")
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg},
            ],
            temperature=0,
        )
        answer = resp.choices[0].message.content.strip()

        # Liczenie token√≥w
        tokens = resp.usage
        print(
            f"[üìä Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]"
        )
        if ENGINE == "openai":
            cost = (
                tokens.prompt_tokens / 1_000_000 * 0.60
                + tokens.completion_tokens / 1_000_000 * 2.40
            )
            print(f"[üí∞ Koszt OpenAI: {cost:.6f} USD]")
        elif ENGINE in {"lmstudio", "anything"}:
            print(f"[üí∞ Model lokalny ({ENGINE}) - brak koszt√≥w]")
        return answer

    elif ENGINE == "claude":
        print(f"[DEBUG] Wysy≈Çam zapytanie do Claude z fragmentami zezna≈Ñ")
        # Claude - bezpo≈õrednia integracja
        resp = claude_client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": system_msg + "\n\n" + user_msg}],
            temperature=0,
            max_tokens=4000,
        )

        # Liczenie token√≥w Claude
        usage = resp.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(
            f"[üìä Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]"
        )
        print(f"[üí∞ Koszt Claude: {cost:.6f} USD]")

        return resp.content[0].text.strip()

    elif ENGINE == "gemini":
        print(f"[DEBUG] Wysy≈Çam zapytanie do Gemini z fragmentami zezna≈Ñ")
        response = model_gemini.generate_content(
            [system_msg, user_msg],
            generation_config={"temperature": 0.0, "max_output_tokens": 512},
        )
        print(f"[üìä Gemini - brak szczeg√≥≈Ç√≥w token√≥w]")
        print(f"[üí∞ Gemini - sprawd≈∫ limity w Google AI Studio]")
        return response.text.strip()


def main():
    print(f"üöÄ U≈ºywam silnika: {ENGINE}")
    base_dir = Path("przesluchania")

    # 1. Pobierz i rozpakuj nagrania tylko je≈õli nie ma plik√≥w audio
    audio_files = find_audio_files(base_dir) if base_dir.exists() else []
    if not audio_files:
        print("1/4 Pobieram i rozpakowujƒô nagrania...")
        download_and_extract_zip(DATA_URL, base_dir)
        audio_files = find_audio_files(base_dir)
    else:
        print("1/4 Pliki audio ju≈º istniejƒÖ, pomijam pobieranie i rozpakowywanie.")

    # 2. Transkrypcja z cache
    transcripts = []
    for audio in audio_files:
        print(f"2/4 Przetwarzanie: {audio.name}")
        text = get_transcript(audio)
        if "arkadiusz" in text.lower():
            continue
        transcripts.append(text)
    combined = "\n".join(transcripts)

    # 3. Ekstrakcja fragment√≥w
    subject = None
    location = None
    for line in combined.split("\n"):
        lw = line.lower()
        if subject is None and ("informatyka" in lw or "matematyka" in lw):
            subject = line.strip()
        if location is None and "krakowie" in lw:
            location = line.strip()
        if subject and location:
            break

    fragments = []
    if subject:
        fragments.append(subject)
    if location:
        fragments.append(location)
    if not fragments:
        last_two = combined.split("\n")[-2:]
        fragments = [l.strip() for l in last_two if l.strip()]
    fragments = "\n".join(fragments)

    print(f"üîç Znalezione fragmenty:\n{fragments}")

    # 4. Wnioskowanie
    print("3/4 Wnioskujƒô z pomocƒÖ LLM...")
    answer = infer_answer(fragments)
    print(f"Odpowied≈∫:\n{answer}")

    # 5. Wys≈Çanie raportu
    print("4/4 Wysy≈Çam raport...")
    payload = {"task": "mp3", "apikey": CENTRALA_KEY, "answer": answer}
    resp = requests.post(REPORT_URL, json=payload)
    if resp.status_code == 200:
        print("‚úÖ Odpowied≈∫ wys≈Çana, serwer odpowiedzia≈Ç:", resp.json())
    else:
        print(f"‚ùå B≈ÇƒÖd przy wysy≈Çaniu: {resp.status_code}\n{resp.text}")


if __name__ == "__main__":
    main()
