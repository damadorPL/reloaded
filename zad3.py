#!/usr/bin/env python3
"""
S01E03  (wersja z konfiguracjÄ… wyÅ‚Ä…cznie z pliku .env)

Pobiera plik kalibracyjny â†’ poprawia â†’ uzupeÅ‚nia brakujÄ…ce odpowiedzi przy pomocy LLM â†’ wysyÅ‚a wynik do Centrali.
Konfiguracja: wszystkie zmienne czytane z .env (bez parametrÃ³w CLI).
DODANO: ObsÅ‚ugÄ™ Claude + liczenie tokenÃ³w i kosztÃ³w dla wszystkich silnikÃ³w (bezpoÅ›rednia integracja)
POPRAWKA: Lepsze wykrywanie silnika z agent.py
"""

from __future__ import annotations
import argparse
import os
import sys
import json
import re
from pathlib import Path

import requests
from dotenv import load_dotenv
from tqdm import tqdm

# Import Claude integration (opcjonalny)
try:
    from claude_integration import setup_claude_for_task, add_token_counting_to_openai_call
except ImportError:
    # Kontynuujemy bez Claude - brak komunikatu o bÅ‚Ä™dzie
    pass

# â”€â”€ 0. Wczytanie konfiguracji (env / .env) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv(override=True)

# â”€â”€ 0.5. CLI argumenty â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="JSON fix + submit (multi-engine)")
parser.add_argument("--engine", choices=["openai", "lmstudio", "anything", "gemini", "claude"],
                    help="LLM backend to use")
args = parser.parse_args()

# â”€â”€ 1. WybÃ³r silnika LLM - POPRAWKA: Lepsze wykrywanie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ENGINE = None
if args.engine:
    ENGINE = args.engine.lower()
elif os.getenv("LLM_ENGINE"):
    ENGINE = os.getenv("LLM_ENGINE").lower()
else:
    # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
    model_name = os.getenv("MODEL_NAME", "")
    if "claude" in model_name.lower():
        ENGINE = "claude"
    elif "gemini" in model_name.lower():
        ENGINE = "gemini"
    elif "gpt" in model_name.lower() or "openai" in model_name.lower():
        ENGINE = "openai"
    else:
        # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
        if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
            ENGINE = "claude"
        elif os.getenv("GEMINI_API_KEY"):
            ENGINE = "gemini"
        elif os.getenv("OPENAI_API_KEY"):
            ENGINE = "openai"
        else:
            ENGINE = "lmstudio"  # domyÅ›lnie

if ENGINE not in {"openai", "lmstudio", "anything", "gemini", "claude"}:
    print(f"âŒ NieobsÅ‚ugiwany silnik: {ENGINE}", file=sys.stderr)
    sys.exit(1)

print(f"ðŸ”„ ENGINE wykryty: {ENGINE}")

# â”€â”€ 2. Konfiguracja Centrali â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CENTRALA_API_KEY = os.getenv("CENTRALA_API_KEY")
if not CENTRALA_API_KEY:
    print("âŒ Brak ustawionej zmiennej CENTRALA_API_KEY", file=sys.stderr)
    sys.exit(1)

REPORT_URL = os.getenv("REPORT_URL")
if not REPORT_URL:
    print("âŒ Brak REPORT_URL w .env", file=sys.stderr)
    sys.exit(1)

SOURCE_URL = os.getenv("SOURCE_URL")
if not SOURCE_URL:
    print("âŒ Brak SOURCE_URL w .env", file=sys.stderr)
    sys.exit(1)

SAVE_FILE = Path(os.getenv("SAVE_FILE", "poprawiony_json.json"))

# â”€â”€ 3. Inicjalizacja klienta LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ENGINE == "openai":
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")
    
    if not OPENAI_API_KEY:
        print("âŒ Brak OPENAI_API_KEY", file=sys.stderr)
        sys.exit(1)
        
    from openai import OpenAI
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_URL)

elif ENGINE == "lmstudio":
    LMSTUDIO_API_KEY = os.getenv("LMSTUDIO_API_KEY", "local")
    LMSTUDIO_API_URL = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_LM", "llama-3.3-70b-instruct")
    print(f"[DEBUG] LMStudio URL: {LMSTUDIO_API_URL}")
    print(f"[DEBUG] LMStudio Model: {MODEL_NAME}")
    from openai import OpenAI
    client = OpenAI(api_key=LMSTUDIO_API_KEY, base_url=LMSTUDIO_API_URL, timeout=60)

elif ENGINE == "anything":
    ANYTHING_API_KEY = os.getenv("ANYTHING_API_KEY", "local")
    ANYTHING_API_URL = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_ANY", "llama-3.3-70b-instruct")
    print(f"[DEBUG] Anything URL: {ANYTHING_API_URL}")
    print(f"[DEBUG] Anything Model: {MODEL_NAME}")
    from openai import OpenAI
    client = OpenAI(api_key=ANYTHING_API_KEY, base_url=ANYTHING_API_URL, timeout=60)

elif ENGINE == "claude":
    # BezpoÅ›rednia integracja Claude
    try:
        from anthropic import Anthropic
    except ImportError:
        print("âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic", file=sys.stderr)
        sys.exit(1)
    
    CLAUDE_API_KEY = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
    if not CLAUDE_API_KEY:
        print("âŒ Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env", file=sys.stderr)
        sys.exit(1)
    
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
    print(f"[DEBUG] Claude Model: {MODEL_NAME}")
    claude_client = Anthropic(api_key=CLAUDE_API_KEY)

elif ENGINE == "gemini":
    import google.generativeai as genai
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    if not GEMINI_API_KEY:
        print("âŒ Brak GEMINI_API_KEY w .env", file=sys.stderr)
        sys.exit(1)
    MODEL_NAME = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
    print(f"[DEBUG] Gemini Model: {MODEL_NAME}")
    genai.configure(api_key=GEMINI_API_KEY)
    model_gemini = genai.GenerativeModel(MODEL_NAME)

print(f"âœ… Zainicjalizowano silnik: {ENGINE} z modelem: {MODEL_NAME}")

# â”€â”€ 4. Pobranie JSONâ€‘a z Centrali â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def download_json(url: str) -> dict[str, Any]:
    print("â¬‡ï¸  Pobieram plik kalibracyjnyâ€¦")
    resp = requests.get(url, timeout=60)
    resp.raise_for_status()
    return resp.json()

# â”€â”€ 5. Prosta arytmetyka w treÅ›ci pytania â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_ARITH = re.compile(r"^\s*(-?\d+)\s*([+\-*/])\s*(-?\d+)\s*$")

def eval_simple_expr(expr: str) -> int | None:
    m = _ARITH.match(expr)
    if not m:
        return None
    a, op, b = int(m.group(1)), m.group(2), int(m.group(3))
    try:
        return {"+": a + b, "-": a - b, "*": a * b, "/": a // b}[op]
    except Exception:
        return None

# â”€â”€ 6. LLM - hurtowe odpowiadanie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_TMPL = (
    "Odpowiedz krÃ³tko na kaÅ¼de pytanie. "
    "ZwrÃ³Ä‡ JSON - listÄ™ odpowiedzi w kolejnoÅ›ci pytaÅ„. "
    "JeÅ›li nie wiesz, zwrÃ³Ä‡ null.\nPytania:\n{qs}\n"
)

def answer_batch(batch: list[dict[str, Any]]) -> None:
    if not batch:
        return
    
    qs = [item["q"] for item in batch]
    prompt = PROMPT_TMPL.format(qs=json.dumps(qs, ensure_ascii=False))
    
    if ENGINE in {"openai", "lmstudio", "anything"}:
        print(f"[DEBUG] WysyÅ‚am zapytanie do {ENGINE} z {len(batch)} pytaniami")
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        raw = response.choices[0].message.content.strip() if response.choices else ""
        
        # Liczenie tokenÃ³w
        tokens = response.usage
        print(f"[ðŸ“Š Prompt: {tokens.prompt_tokens} | Completion: {tokens.completion_tokens} | Total: {tokens.total_tokens}]")
        if ENGINE == "openai":
            cost = tokens.prompt_tokens/1_000_000*0.60 + tokens.completion_tokens/1_000_000*2.40
            print(f"[ðŸ’° Koszt OpenAI: {cost:.6f} USD]")
        elif ENGINE in {"lmstudio", "anything"}:
            print(f"[ðŸ’° Model lokalny ({ENGINE}) - brak kosztÃ³w]")
        
    elif ENGINE == "claude":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Claude z {len(batch)} pytaniami")
        # Claude - bezpoÅ›rednia integracja
        response = claude_client.messages.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=4000
        )
        raw = response.content[0].text.strip()
        
        # Liczenie tokenÃ³w Claude
        usage = response.usage
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015  # Claude Sonnet 4 pricing
        print(f"[ðŸ“Š Prompt: {usage.input_tokens} | Completion: {usage.output_tokens} | Total: {usage.input_tokens + usage.output_tokens}]")
        print(f"[ðŸ’° Koszt Claude: {cost:.6f} USD]")
        
    elif ENGINE == "gemini":
        print(f"[DEBUG] WysyÅ‚am zapytanie do Gemini z {len(batch)} pytaniami")
        response = model_gemini.generate_content(
            [prompt],
            generation_config={"temperature": 0.0, "max_output_tokens": 512}
        )
        raw = response.text.strip()
        print(f"[ðŸ“Š Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
        print(f"[ðŸ’° Gemini - sprawdÅº limity w Google AI Studio]")
    
    # Przetwarzanie odpowiedzi (ta sama logika dla wszystkich silnikÃ³w)
    raw = re.sub(r"^```[a-zA-Z]*|```$", "", raw, flags=re.MULTILINE).strip()
    try:
        answers = json.loads(raw)
        if not isinstance(answers, list):
            raise ValueError
    except Exception:
        found = re.search(r"\[.*?\]", raw, flags=re.S)
        answers = json.loads(found.group(0)) if found else []
    
    answers += [None] * max(0, len(batch) - len(answers))
    for rec, ans in zip(batch, answers[: len(batch)]):
        rec["a"] = ans

# â”€â”€ 7. Transformacja danych (krok po kroku) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fix_data(data: dict[str, Any]) -> dict[str, Any]:
    print("âš™ï¸  Naprawiam daneâ€¦")
    data["apikey"] = CENTRALA_API_KEY
    raw_td = data.get("test-data", [])
    td = []
    if isinstance(raw_td, dict):
        td = [v for v in raw_td.values() if isinstance(v, dict)]
    elif isinstance(raw_td, list):
        td = raw_td
    
    batch: list[dict[str, Any]] = []
    
    for rec in td:
        if q := rec.get("question"):
            if result := eval_simple_expr(q):
                rec["answer"] = result
        if test := rec.get("test"):
            batch.append(test)
            if len(batch) >= 90:
                answer_batch(batch)
                batch.clear()
    
    answer_batch(batch)
    data["test-data"] = td
    return data

# â”€â”€ 8. WysÅ‚anie raportu do Centrali â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def submit_report(answer: dict[str, Any]) -> None:
    print("ðŸ“¡ WysyÅ‚am raportâ€¦")
    payload = {"task": "JSON", "apikey": CENTRALA_API_KEY, "answer": answer}
    resp = requests.post(REPORT_URL, json=payload, timeout=60)
    if resp.ok:
        print("ðŸŽ‰ Sukces! OdpowiedÅº serwera:", resp.json())
    else:
        print(f"âŒ BÅ‚Ä…d HTTP {resp.status_code}: {resp.text}", file=sys.stderr)

# â”€â”€ 9. GÅ‚Ã³wna logika â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    print(f"ðŸš€ UÅ¼ywam silnika: {ENGINE}")
    original = download_json(SOURCE_URL)
    fixed = fix_data(original)
    SAVE_FILE.write_text(json.dumps(fixed, ensure_ascii=False, indent=2), encoding="utf-8")
    print("ðŸ’¾ Zapisano lokalnie â†’", SAVE_FILE)
    submit_report(fixed)

if __name__ == "__main__":
    main()